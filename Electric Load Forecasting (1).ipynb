{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder\n",
    "\n",
    "This is a 4 layers Autoencoder(2 encoder + 2 decoder) for performing dimensionality reduction on the GEFCOM-2014 load forecasting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date, timedelta, datetime\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "CUDA_ENABLED = True\n",
    "\n",
    "if CUDA_ENABLED:\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing & Staging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(file_name):\n",
    "    data = pd.read_csv(file_name)\n",
    "    data = data.to_numpy()\n",
    "    for i in range(data.shape[0]):\n",
    "        data[i, 1] = datetime.strptime(data[i, 1], \"%m%d%Y %H:%M\").date()\n",
    "    power = data[:, 2].astype(np.float64)\n",
    "    indexes = np.isnan(power)\n",
    "    cleaned_data = data[~indexes, :]\n",
    "    return cleaned_data\n",
    "\n",
    "def embed_data(lead_time, embed_dim, from_date, to_date, data, t_idx, dt_idx):\n",
    "    start = np.where(data[:, dt_idx] == from_date)[0][0]\n",
    "    end = np.where(data[:, dt_idx] == to_date)[0][-1]\n",
    "    X, y = [], []\n",
    "    for idx in range(start, end+1):\n",
    "        xx = data[idx-(embed_dim+lead_time-1):idx-lead_time+1, 3:]\n",
    "        yy = data[idx, 2]\n",
    "        X.append(xx)\n",
    "        y.append(yy)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def get_data_between_time(data, from_date, to_date, dt_idx):\n",
    "    start = np.where(data[:, dt_idx] == from_date)[0][0]\n",
    "    end = np.where(data[:, dt_idx] == to_date)[0][-1]\n",
    "    return data[start:(end+1), :]\n",
    "\n",
    "\n",
    "def normalize(data):\n",
    "    mean_out, stds_out = 0, 0\n",
    "    for i in range(2, data.shape[1]):\n",
    "        if i == 2:\n",
    "            mean_out = np.mean(data[:, i])\n",
    "            stds_out = np.std(data[:, i])\n",
    "        data[:, i] = (data[:, i] - np.mean(data[:, i]))/np.std(data[:, i])\n",
    "    return data, mean_out, stds_out\n",
    "\n",
    "def denormalize(y, mean, std):\n",
    "    return y*std + mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data('data/task_1/L1-train.csv')\n",
    "data, mean_out, stds_out = normalize(data)\n",
    "embedded_data, target = embed_data(\n",
    "    lead_time = 6,\n",
    "    embed_dim = 12,\n",
    "    from_date = date(month=2, year=2005, day=1),\n",
    "    to_date = date(month=2, year=2006, day=1),\n",
    "    data = data,\n",
    "    t_idx = 2,\n",
    "    dt_idx = 1\n",
    ")\n",
    "\n",
    "train_x = embedded_data.astype(np.float32)\n",
    "train_y = target.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size, down_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.down_size = down_size\n",
    "        \n",
    "        self.factor = input_size / down_size\n",
    "        self.encoded_size = self.down_size\n",
    "        \n",
    "        self.factor_root = np.sqrt(self.factor)\n",
    "        self.first_squash = int(input_size / self.factor_root)\n",
    "        \n",
    "        self.linear_encode1 = nn.Linear(self.input_size, self.first_squash)\n",
    "        self.linear_encode2 = nn.Linear(self.first_squash, self.encoded_size)\n",
    "        \n",
    "        self.linear_decode1 = nn.Linear(self.encoded_size, self.first_squash)\n",
    "        self.linear_decode2 = nn.Linear(self.first_squash, self.input_size)\n",
    "        \n",
    "        # Activation\n",
    "        self.first_squash_fn = nn.ELU()\n",
    "        self.encode_fn = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, xx, decode = True, no_act = False):\n",
    "        if no_act:\n",
    "            xx = self.linear_encode1(xx)\n",
    "            xx = self.linear_encode2(xx)\n",
    "\n",
    "            if decode : \n",
    "                xx = self.linear_decode1(xx)\n",
    "                xx = self.linear_decode2(xx) \n",
    "        else:\n",
    "            xx = self.first_squash_fn( self.linear_encode1(xx) )\n",
    "            xx = self.encode_fn(self.linear_encode2(xx))\n",
    "\n",
    "            if decode : \n",
    "                xx = self.encode_fn(self.linear_decode1(xx))\n",
    "                xx = self.first_squash_fn( self.linear_decode2(xx) )   \n",
    "                \n",
    "        return xx\n",
    "    \n",
    "    def train(self, data, batch_size, epochs = 30, lr = 0.01, no_act = False):\n",
    "        learning_rate = 0.01\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr = lr)\n",
    "        losses = []\n",
    "        # Train the model\n",
    "        for epoch in range(epochs):  \n",
    "            indices = torch.randperm(data.size()[0])\n",
    "            data = data[indices, :]\n",
    "            for i in range(0, data.size()[0], batch_size):\n",
    "                xx = data[i: i + batch_size, :]\n",
    "                outputs = self.forward(xx, no_act = no_act)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                loss = criterion(outputs, xx)\n",
    "                losses.append(loss.item())\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item()))\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 1.22783\n",
      "Epoch: 1, loss: 1.04645\n",
      "Epoch: 2, loss: 0.61382\n",
      "Epoch: 3, loss: 0.92990\n",
      "Epoch: 4, loss: 0.94632\n",
      "Epoch: 5, loss: 0.58952\n",
      "Epoch: 6, loss: 0.49251\n",
      "Epoch: 7, loss: 0.40487\n",
      "Epoch: 8, loss: 0.39773\n",
      "Epoch: 9, loss: 0.37141\n",
      "Epoch: 10, loss: 0.26149\n",
      "Epoch: 11, loss: 0.21700\n",
      "Epoch: 12, loss: 0.22386\n",
      "Epoch: 13, loss: 0.15706\n",
      "Epoch: 14, loss: 0.12884\n",
      "Epoch: 15, loss: 0.14064\n",
      "Epoch: 16, loss: 0.08796\n",
      "Epoch: 17, loss: 0.08468\n",
      "Epoch: 18, loss: 0.07430\n",
      "Epoch: 19, loss: 0.10132\n",
      "Epoch: 20, loss: 0.06668\n",
      "Epoch: 21, loss: 0.05326\n",
      "Epoch: 22, loss: 0.07082\n",
      "Epoch: 23, loss: 0.05989\n",
      "Epoch: 24, loss: 0.04534\n",
      "Epoch: 25, loss: 0.04767\n",
      "Epoch: 26, loss: 0.05678\n",
      "Epoch: 27, loss: 0.05464\n",
      "Epoch: 28, loss: 0.05705\n",
      "Epoch: 29, loss: 0.05148\n",
      "Epoch: 30, loss: 0.06468\n",
      "Epoch: 31, loss: 0.03992\n",
      "Epoch: 32, loss: 0.04508\n",
      "Epoch: 33, loss: 0.03730\n",
      "Epoch: 34, loss: 0.03952\n",
      "Epoch: 35, loss: 0.04627\n",
      "Epoch: 36, loss: 0.04480\n",
      "Epoch: 37, loss: 0.03165\n",
      "Epoch: 38, loss: 0.03360\n",
      "Epoch: 39, loss: 0.04928\n",
      "Epoch: 40, loss: 0.04546\n",
      "Epoch: 41, loss: 0.04362\n",
      "Epoch: 42, loss: 0.05291\n",
      "Epoch: 43, loss: 0.03974\n",
      "Epoch: 44, loss: 0.03918\n",
      "Epoch: 45, loss: 0.05657\n",
      "Epoch: 46, loss: 0.04771\n",
      "Epoch: 47, loss: 0.05296\n",
      "Epoch: 48, loss: 0.06553\n",
      "Epoch: 49, loss: 0.04936\n",
      "Epoch: 50, loss: 0.04014\n",
      "Epoch: 51, loss: 0.03965\n",
      "Epoch: 52, loss: 0.04039\n",
      "Epoch: 53, loss: 0.04101\n",
      "Epoch: 54, loss: 0.03949\n",
      "Epoch: 55, loss: 0.04909\n",
      "Epoch: 56, loss: 0.04153\n",
      "Epoch: 57, loss: 0.03529\n",
      "Epoch: 58, loss: 0.05000\n",
      "Epoch: 59, loss: 0.03757\n",
      "Epoch: 60, loss: 0.03593\n",
      "Epoch: 61, loss: 0.03794\n",
      "Epoch: 62, loss: 0.03940\n",
      "Epoch: 63, loss: 0.04485\n",
      "Epoch: 64, loss: 0.03641\n",
      "Epoch: 65, loss: 0.03973\n",
      "Epoch: 66, loss: 0.03857\n",
      "Epoch: 67, loss: 0.03099\n",
      "Epoch: 68, loss: 0.03214\n",
      "Epoch: 69, loss: 0.04771\n",
      "Epoch: 70, loss: 0.02432\n",
      "Epoch: 71, loss: 0.04370\n",
      "Epoch: 72, loss: 0.04160\n",
      "Epoch: 73, loss: 0.02789\n",
      "Epoch: 74, loss: 0.05362\n",
      "Epoch: 75, loss: 0.05329\n",
      "Epoch: 76, loss: 0.03037\n",
      "Epoch: 77, loss: 0.03025\n",
      "Epoch: 78, loss: 0.03139\n",
      "Epoch: 79, loss: 0.02711\n",
      "Epoch: 80, loss: 0.03453\n",
      "Epoch: 81, loss: 0.03186\n",
      "Epoch: 82, loss: 0.02377\n",
      "Epoch: 83, loss: 0.03601\n",
      "Epoch: 84, loss: 0.02270\n",
      "Epoch: 85, loss: 0.03197\n",
      "Epoch: 86, loss: 0.02496\n",
      "Epoch: 87, loss: 0.02863\n",
      "Epoch: 88, loss: 0.02488\n",
      "Epoch: 89, loss: 0.03498\n",
      "Epoch: 90, loss: 0.03278\n",
      "Epoch: 91, loss: 0.02265\n",
      "Epoch: 92, loss: 0.02670\n",
      "Epoch: 93, loss: 0.03597\n",
      "Epoch: 94, loss: 0.02416\n",
      "Epoch: 95, loss: 0.02591\n",
      "Epoch: 96, loss: 0.02601\n",
      "Epoch: 97, loss: 0.03316\n",
      "Epoch: 98, loss: 0.03106\n",
      "Epoch: 99, loss: 0.02955\n",
      "Epoch: 100, loss: 0.02894\n",
      "Epoch: 101, loss: 0.02621\n",
      "Epoch: 102, loss: 0.02240\n",
      "Epoch: 103, loss: 0.02362\n",
      "Epoch: 104, loss: 0.01884\n",
      "Epoch: 105, loss: 0.02786\n",
      "Epoch: 106, loss: 0.02826\n",
      "Epoch: 107, loss: 0.02172\n",
      "Epoch: 108, loss: 0.02758\n",
      "Epoch: 109, loss: 0.02538\n",
      "Epoch: 110, loss: 0.02159\n",
      "Epoch: 111, loss: 0.02876\n",
      "Epoch: 112, loss: 0.01887\n",
      "Epoch: 113, loss: 0.02258\n",
      "Epoch: 114, loss: 0.02248\n",
      "Epoch: 115, loss: 0.02477\n",
      "Epoch: 116, loss: 0.02878\n",
      "Epoch: 117, loss: 0.02545\n",
      "Epoch: 118, loss: 0.02101\n",
      "Epoch: 119, loss: 0.02789\n",
      "Epoch: 120, loss: 0.02402\n",
      "Epoch: 121, loss: 0.01988\n",
      "Epoch: 122, loss: 0.01974\n",
      "Epoch: 123, loss: 0.02666\n",
      "Epoch: 124, loss: 0.02939\n",
      "Epoch: 125, loss: 0.01894\n",
      "Epoch: 126, loss: 0.02490\n",
      "Epoch: 127, loss: 0.01912\n",
      "Epoch: 128, loss: 0.02088\n",
      "Epoch: 129, loss: 0.02021\n",
      "Epoch: 130, loss: 0.02376\n",
      "Epoch: 131, loss: 0.02309\n",
      "Epoch: 132, loss: 0.01977\n",
      "Epoch: 133, loss: 0.02270\n",
      "Epoch: 134, loss: 0.02063\n",
      "Epoch: 135, loss: 0.03180\n",
      "Epoch: 136, loss: 0.02268\n"
     ]
    }
   ],
   "source": [
    "ae_data = get_data_between_time(\n",
    "    data = data,\n",
    "    from_date = date(month=1, year=2005, day=1),\n",
    "    to_date = date(month=12, year=2005, day=31),\n",
    "    dt_idx=1\n",
    ")\n",
    "\n",
    "ae_data = Variable(torch.from_numpy(ae_data[:, 3:].astype(np.float32))).cuda()\n",
    "\n",
    "autoencoder = Autoencoder(\n",
    "    input_size = 25,\n",
    "    down_size = 15\n",
    ")\n",
    "\n",
    "losses = autoencoder.train(\n",
    "    data = ae_data, \n",
    "    batch_size = 20, \n",
    "    epochs = 200, \n",
    "    lr = 0.000005,\n",
    "    no_act = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(autoencoder.state_dict(), './data/autoencoder_params')\n",
    "\n",
    "# new_autoencoder = Autoencoder(\n",
    "#     input_size = 25,\n",
    "#     down_size = 15\n",
    "# )\n",
    "\n",
    "# new_autoencoder.load_state_dict(torch.load('./data/autoencoder_params'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Autoencoder Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Reduced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTensor = Variable(torch.from_numpy(data[:, 3:].astype(np.float32))).cuda()\n",
    "encoded_data_without_date = autoencoder.forward(dataTensor, decode=False, no_act=True).cpu().detach().numpy()\n",
    "encoded_data = np.concatenate((data[:, :3], encoded_data_without_date), axis=1)\n",
    "embedded_encoded_data, target = embed_data(\n",
    "    lead_time = 6,\n",
    "    embed_dim = 12,\n",
    "    from_date = date(month=1, year=2006, day=1),\n",
    "    to_date = date(month=3, year=2006, day=31),\n",
    "    data = encoded_data,\n",
    "    t_idx = 2,\n",
    "    dt_idx = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM for forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantileRegression(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size, num_layers, seq_length, alpha, bidirectional=True):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_length = seq_length\n",
    "        self.num_layers = num_layers\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=input_size, \n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, \n",
    "                            batch_first=True, bidirectional=False)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, 10)\n",
    "        self.fc_out = nn.Linear(10, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_0 = Variable(torch.zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_size))\n",
    "        \n",
    "        c_0 = Variable(torch.zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_size))\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "        # Propagate input through LSTM\n",
    "        ula, (h_out, _) = self.lstm(x, (h_0, c_0))\n",
    "        h_out = h_out.view(batch_size*self.num_layers, self.hidden_size)\n",
    "        \n",
    "        fc1 = self.fc(h_out)\n",
    "        out = self.fc_out(fc1)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def train_model(self, trainX, trainY, lr, epochs, sample_size, verbose=True):\n",
    "        learning_rate = lr\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        losses = []\n",
    "        num_epochs = epochs\n",
    "        \n",
    "        for epoch in range(num_epochs):  \n",
    "            indices = torch.randperm(trainX.size()[0])\n",
    "            trainX, trainY = trainX[indices, :, :], trainY[indices]\n",
    "            for i in range(0, trainX.size()[0], sample_size):\n",
    "                xx = trainX[i: i + sample_size, :, :]\n",
    "                yy = trainY[i: i + sample_size]\n",
    "                outputs = self.forward(xx)\n",
    "                outputs = outputs.view(self.num_layers, self.output_size, -1)\n",
    "                optimizer.zero_grad()\n",
    "                alpha = self.alpha\n",
    "                # obtain the loss function\n",
    "                loss = self.quantile_loss(outputs[-1, :, :], yy)\n",
    "                losses.append(loss.item())\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "            if verbose:\n",
    "                print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item()))\n",
    "            \n",
    "        return losses\n",
    "    \n",
    "    def train_sgd(self, trainX, trainY, lr, epochs, sample_size):\n",
    "        learning_rate = lr\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        losses = []\n",
    "        num_epochs = epochs\n",
    "        for epoch in range(num_epochs):\n",
    "            for xx, yy in zip(trainX, trainY):\n",
    "                xx = xx.view(1, self.seq_length, self.input_size)\n",
    "                outputs = self.forward(xx)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # obtain the loss function\n",
    "                loss = self.quantile_loss(outputs, yy)\n",
    "                losses.append(loss)\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "            print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item()))\n",
    "        return losses\n",
    "    \n",
    "    def quantile_loss(self, output, target):\n",
    "        covered_flag = (output <= target).float()\n",
    "        uncovered_flag = (output > target).float()\n",
    "        return torch.mean((target - output)*(self.alpha)*covered_flag + (output-target)*(1-self.alpha)*uncovered_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_loss(output, target, alpha):\n",
    "    covered_flag = (output <= target)\n",
    "    uncovered_flag = (output > target)\n",
    "    return np.mean((target - output)*(alpha)*covered_flag + (output-target)*(1-alpha)*uncovered_flag)\n",
    "\n",
    "lead_time = [6, 12]\n",
    "embed_dims = [12, 24, 36]\n",
    "quantiles = np.arange(0.05, 1.0, 0.05)\n",
    "\n",
    "for l in lead_time:\n",
    "    for e in embed_dims:\n",
    "        embedded_data, target = embed_data(\n",
    "            lead_time = l,\n",
    "            embed_dim = e,\n",
    "            from_date = date(month=2, year=2005, day=1),\n",
    "            to_date = date(month=1, year=2006, day=31),\n",
    "            data = encoded_data,\n",
    "            t_idx = 2,\n",
    "            dt_idx = 1\n",
    "        )\n",
    "\n",
    "        train_x = embedded_data.astype(np.float32)\n",
    "        train_y = target.astype(np.float32)\n",
    "\n",
    "        trainX = Variable(torch.from_numpy(train_x)).cuda()\n",
    "        trainY = Variable(torch.from_numpy(train_y)).cuda()\n",
    "        \n",
    "        embedded_data_test, target_test = embed_data(\n",
    "            lead_time = l,\n",
    "            embed_dim = e,\n",
    "            from_date = date(month=2, year=2006, day=1),\n",
    "            to_date = date(month=1, year=2007, day=31),\n",
    "            data = encoded_data,\n",
    "            t_idx = 2,\n",
    "            dt_idx = 1\n",
    "        )\n",
    "\n",
    "        test_x = embedded_data_test.astype(np.float32)\n",
    "        test_y = target_test.astype(np.float32)\n",
    "\n",
    "        testX = Variable(torch.from_numpy(test_x)).cuda()\n",
    "        testY = Variable(torch.from_numpy(test_y)).cuda()\n",
    "        crps = []\n",
    "        for q in tqdm(quantiles):\n",
    "            regressor = QuantileRegression(\n",
    "                input_size = 15,\n",
    "                hidden_size = 40,\n",
    "                output_size = 1,\n",
    "                num_layers = 1,\n",
    "                seq_length = e,\n",
    "                alpha=q\n",
    "            )\n",
    "\n",
    "            losses = regressor.train_model(\n",
    "                trainX = trainX,\n",
    "                trainY = trainY,\n",
    "                lr = 0.005,\n",
    "                epochs = 200,\n",
    "                sample_size = 50,\n",
    "                verbose = False\n",
    "            )\n",
    "            \n",
    "            regressor.eval()\n",
    "            predY = regressor(testX).data.cpu().detach().numpy()\n",
    "            predY = denormalize(predY, mean_out, stds_out)\n",
    "            testYDenorm = denormalize(test_y, mean_out, stds_out)\n",
    "            crps.append(quantile_loss(predY, testYDenorm, q))\n",
    "            print(crps[-1])\n",
    "        print(\"Error for Lead Time : \", l, \"Embed Dim: \", e, \" CRPS: \", sum(crps)/len(crps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.eval()\n",
    "train_predict = regressor(trainX)\n",
    "data_predict = train_predict.data.cpu().detach().numpy()\n",
    "dataY_plot =  trainY.data.cpu().detach().numpy()\n",
    "print(data_predict.shape, dataY_plot.shape)\n",
    "\n",
    "figure(num=None, figsize=(20, 8), dpi=80, facecolor='w', edgecolor='k')\n",
    "layers  = 1\n",
    "from_here = int(data_predict.shape[0] / layers)\n",
    "plt.plot(dataY_plot[:200])\n",
    "plt.plot(data_predict[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Check post Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_reduced, train_target_reduced = embed_data(\n",
    "    lead_time = 6,\n",
    "    embed_dim = 12,\n",
    "    from_date = date(month=2, year=2006, day=1),\n",
    "    to_date = date(month=4, year=2006, day=30),\n",
    "    data = encoded_data,\n",
    "    t_idx = 2,\n",
    "    dt_idx = 1\n",
    ")\n",
    "\n",
    "\n",
    "regressor_reduced_dim = QuantileRegression(\n",
    "    input_size=15,\n",
    "    output_size=1,\n",
    "    hidden_size=40,\n",
    "    num_layers=1,\n",
    "    seq_length=12,\n",
    "    alpha=0.9\n",
    ")\n",
    "\n",
    "train_x = train_data_reduced.astype(np.float32)\n",
    "train_y = train_target_reduced.astype(np.float32)\n",
    "\n",
    "trainX = Variable(torch.from_numpy(train_x))\n",
    "trainY = Variable(torch.from_numpy(train_y))\n",
    "\n",
    "losses = regressor_reduced_dim.train_model(\n",
    "    trainX = trainX,\n",
    "    trainY = trainY,\n",
    "    lr = 0.005,\n",
    "    epochs = 100,\n",
    "    sample_size = 50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_original, train_target_original = embed_data(\n",
    "    lead_time = 6,\n",
    "    embed_dim = 12,\n",
    "    from_date = date(month=2, year=2006, day=1),\n",
    "    to_date = date(month=4, year=2006, day=30),\n",
    "    data = data,\n",
    "    t_idx = 2,\n",
    "    dt_idx = 1\n",
    ")\n",
    "\n",
    "regressor_original_dim = QuantileRegression(\n",
    "    input_size=25,\n",
    "    output_size=1,\n",
    "    hidden_size=40,\n",
    "    num_layers=1,\n",
    "    seq_length=12,\n",
    "    alpha = 0.9\n",
    ")\n",
    "\n",
    "train_x = train_data_original.astype(np.float32)\n",
    "train_y = train_target_original.astype(np.float32)\n",
    "\n",
    "trainX = Variable(torch.from_numpy(train_x))\n",
    "trainY = Variable(torch.from_numpy(train_y))\n",
    "\n",
    "losses = regressor_original_dim.train_model(\n",
    "    trainX = trainX,\n",
    "    trainY = trainY,\n",
    "    lr = 0.005,\n",
    "    epochs = 100,\n",
    "    sample_size = 50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_data_test, target_test = embed_data(\n",
    "    lead_time = 6,\n",
    "    embed_dim = 12,\n",
    "    from_date = date(month=5, year=2006, day=1),\n",
    "    to_date = date(month=6, year=2006, day=30),\n",
    "    data = data,\n",
    "    t_idx = 2,\n",
    "    dt_idx = 1\n",
    ")\n",
    "test_x_original = embedded_data_test.astype(np.float32)\n",
    "test_y_original = target_test.astype(np.float32)\n",
    "\n",
    "testXOriginal = Variable(torch.from_numpy(test_x_original))\n",
    "\n",
    "embedded_encoded_data_test, target_test = embed_data(\n",
    "    lead_time = 6,\n",
    "    embed_dim = 12,\n",
    "    from_date = date(month=5, year=2006, day=1),\n",
    "    to_date = date(month=6, year=2006, day=30),\n",
    "    data = encoded_data,\n",
    "    t_idx = 2,\n",
    "    dt_idx = 1\n",
    ")\n",
    "\n",
    "test_x_reduced = embedded_encoded_data_test.astype(np.float32)\n",
    "test_y_reduced = target_test.astype(np.float32)\n",
    "\n",
    "testXReduced = Variable(torch.from_numpy(test_x_reduced))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor_original_dim.eval()\n",
    "testYOriginalPredict = regressor_original_dim(testXOriginal)\n",
    "test_pred_original = testYOriginalPredict.data.numpy()\n",
    "\n",
    "regressor_reduced_dim.eval()\n",
    "testYReducedPredict = regressor_reduced_dim(testXReduced)\n",
    "test_pred_reduced = testYReducedPredict.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_loss(output, target, alpha):\n",
    "    covered_flag = (output <= target).astype(np.float32)\n",
    "    uncovered_flag = (output > target).astype(np.float32)\n",
    "    return np.mean((target - output)*(alpha)*covered_flag + (output-target)*(1-alpha)*uncovered_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(quantile_loss(test_pred_original, test_y_original, alpha=0.9))\n",
    "print(quantile_loss(test_pred_reduced, test_y_reduced, alpha=0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0.2, 0.4, 0.6, 0.8]\n",
    "\n",
    "trainX_original = Variable(torch.from_numpy(train_data_original.astype(np.float32)))\n",
    "trainX_reduced = Variable(torch.from_numpy(train_data_reduced.astype(np.float32)))\n",
    "trainY = Variable(torch.from_numpy(train_target_reduced.astype(np.float32)))\n",
    "\n",
    "for alpha in alphas:\n",
    "    \n",
    "    regressor_original_dim = QuantileRegression(\n",
    "        input_size=25,\n",
    "        output_size=1,\n",
    "        hidden_size=40,\n",
    "        num_layers=1,\n",
    "        seq_length=12,\n",
    "        alpha = alpha\n",
    "    )\n",
    "\n",
    "    regressor_reduced_dim = QuantileRegression(\n",
    "        input_size=15,\n",
    "        output_size=1,\n",
    "        hidden_size=40,\n",
    "        num_layers=1,\n",
    "        seq_length=12,\n",
    "        alpha=alpha\n",
    "    )\n",
    "\n",
    "    losses = regressor_original_dim.train_model(\n",
    "        trainX = trainX_original,\n",
    "        trainY = trainY,\n",
    "        lr = 0.005,\n",
    "        epochs = 100,\n",
    "        sample_size = 50,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    losses = regressor_reduced_dim.train_model(\n",
    "        trainX = trainX_reduced,\n",
    "        trainY = trainY,\n",
    "        lr = 0.005,\n",
    "        epochs = 100,\n",
    "        sample_size = 50,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    regressor_original_dim.eval()\n",
    "    testYOriginalPredict = regressor_original_dim(testXOriginal)\n",
    "    test_pred_original = testYOriginalPredict.data.numpy()\n",
    "\n",
    "    regressor_reduced_dim.eval()\n",
    "    testYReducedPredict = regressor_reduced_dim(testXReduced)\n",
    "    test_pred_reduced = testYReducedPredict.data.numpy()\n",
    "\n",
    "    print('These are the losses before and after dimensionality reduction to 15 variables')\n",
    "    print('Before : ', quantile_loss(test_pred_original, test_y_original, alpha=alpha))\n",
    "    print('After : ', quantile_loss(test_pred_reduced, test_y_reduced, alpha=alpha))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
