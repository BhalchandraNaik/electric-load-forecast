{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder\n",
    "\n",
    "This is a 4 layers Autoencoder(2 encoder + 2 decoder) for performing dimensionality reduction on the GEFCOM-2014 load forecasting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date, timedelta, datetime\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from matplotlib.pyplot import figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing & Staging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(file_name):\n",
    "    data = pd.read_csv(file_name)\n",
    "    data = data.to_numpy()\n",
    "    for i in range(data.shape[0]):\n",
    "        data[i, 1] = datetime.strptime(data[i, 1], \"%m%d%Y %H:%M\").date()\n",
    "    power = data[:, 2].astype(np.float64)\n",
    "    indexes = np.isnan(power)\n",
    "    cleaned_data = data[~indexes, :]\n",
    "    return cleaned_data\n",
    "\n",
    "def embed_data(lead_time, embed_dim, from_date, to_date, data, t_idx, dt_idx):\n",
    "    start = np.where(data[:, dt_idx] == from_date)[0][0]\n",
    "    end = np.where(data[:, dt_idx] == to_date)[0][-1]\n",
    "    X, y = [], []\n",
    "    for idx in range(start, end+1):\n",
    "        xx = data[idx-(embed_dim+lead_time-1):idx-lead_time+1, 3:]\n",
    "        yy = data[idx, 2]\n",
    "        X.append(xx)\n",
    "        y.append(yy)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def get_data_between_time(data, from_date, to_date, dt_idx):\n",
    "    start = np.where(data[:, dt_idx] == from_date)[0][0]\n",
    "    end = np.where(data[:, dt_idx] == to_date)[0][-1]\n",
    "    return data[start:(end+1), :]\n",
    "\n",
    "\n",
    "def normalize(data):\n",
    "    for i in range(2, data.shape[1]):\n",
    "        data[:, i] = (data[:, i] - np.mean(data[:, i]))/np.std(data[:, i])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data('./data/task_1/L1-train.csv')\n",
    "data = normalize(data)\n",
    "embedded_data, target = embed_data(\n",
    "    lead_time = 6,\n",
    "    embed_dim = 12,\n",
    "    from_date = date(month=2, year=2005, day=1),\n",
    "    to_date = date(month=4, year=2005, day=30),\n",
    "    data = data,\n",
    "    t_idx = 2,\n",
    "    dt_idx = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size, down_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.down_size = down_size\n",
    "        \n",
    "        self.factor = input_size / down_size\n",
    "        self.encoded_size = self.down_size\n",
    "        \n",
    "        self.factor_root = np.sqrt(self.factor)\n",
    "        self.first_squash = int(input_size / self.factor_root)\n",
    "        \n",
    "        self.linear_encode1 = nn.Linear(self.input_size, self.first_squash)\n",
    "        self.linear_encode2 = nn.Linear(self.first_squash, self.encoded_size)\n",
    "        \n",
    "        self.linear_decode1 = nn.Linear(self.encoded_size, self.first_squash)\n",
    "        self.linear_decode2 = nn.Linear(self.first_squash, self.input_size)\n",
    "        \n",
    "        # Activation\n",
    "        self.first_squash_fn = nn.ELU()\n",
    "        self.encode_fn = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, xx, decode = True, no_act = False):\n",
    "        if no_act:\n",
    "            xx = self.linear_encode1(xx)\n",
    "            xx = self.linear_encode2(xx)\n",
    "\n",
    "            if decode : \n",
    "                xx = self.linear_decode1(xx)\n",
    "                xx = self.linear_decode2(xx) \n",
    "        else:\n",
    "            xx = self.first_squash_fn( self.linear_encode1(xx) )\n",
    "            xx = self.encode_fn(self.linear_encode2(xx))\n",
    "\n",
    "            if decode : \n",
    "                xx = self.encode_fn(self.linear_decode1(xx))\n",
    "                xx = self.first_squash_fn( self.linear_decode2(xx) )   \n",
    "                \n",
    "        return xx\n",
    "    \n",
    "    def train(self, data, batch_size, epochs = 30, lr = 0.01, no_act = False):\n",
    "        learning_rate = 0.01\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr = lr)\n",
    "        losses = []\n",
    "        # Train the model\n",
    "        for epoch in range(epochs):  \n",
    "            indices = torch.randperm(data.size()[0])\n",
    "            data = data[indices, :]\n",
    "            for i in range(0, data.size()[0], batch_size):\n",
    "                xx = data[i: i + batch_size, :]\n",
    "                outputs = self.forward(xx, no_act = no_act)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                loss = criterion(outputs, xx)\n",
    "                losses.append(loss.item())\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item()))\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 1.13016\n",
      "Epoch: 1, loss: 0.99306\n",
      "Epoch: 2, loss: 1.52001\n",
      "Epoch: 3, loss: 1.12809\n",
      "Epoch: 4, loss: 0.63293\n",
      "Epoch: 5, loss: 0.97349\n",
      "Epoch: 6, loss: 0.71205\n",
      "Epoch: 7, loss: 0.68225\n",
      "Epoch: 8, loss: 0.56654\n",
      "Epoch: 9, loss: 0.49833\n",
      "Epoch: 10, loss: 0.49267\n",
      "Epoch: 11, loss: 0.35852\n",
      "Epoch: 12, loss: 0.31349\n",
      "Epoch: 13, loss: 0.33022\n",
      "Epoch: 14, loss: 0.23313\n",
      "Epoch: 15, loss: 0.14945\n",
      "Epoch: 16, loss: 0.16328\n",
      "Epoch: 17, loss: 0.10911\n",
      "Epoch: 18, loss: 0.11331\n",
      "Epoch: 19, loss: 0.09492\n",
      "Epoch: 20, loss: 0.06741\n",
      "Epoch: 21, loss: 0.06959\n",
      "Epoch: 22, loss: 0.06522\n",
      "Epoch: 23, loss: 0.06770\n",
      "Epoch: 24, loss: 0.08926\n",
      "Epoch: 25, loss: 0.07173\n",
      "Epoch: 26, loss: 0.06039\n",
      "Epoch: 27, loss: 0.06844\n",
      "Epoch: 28, loss: 0.04619\n",
      "Epoch: 29, loss: 0.05796\n",
      "Epoch: 30, loss: 0.05649\n",
      "Epoch: 31, loss: 0.04856\n",
      "Epoch: 32, loss: 0.03773\n",
      "Epoch: 33, loss: 0.05568\n",
      "Epoch: 34, loss: 0.03286\n",
      "Epoch: 35, loss: 0.04428\n",
      "Epoch: 36, loss: 0.04804\n",
      "Epoch: 37, loss: 0.05483\n",
      "Epoch: 38, loss: 0.05137\n",
      "Epoch: 39, loss: 0.04322\n",
      "Epoch: 40, loss: 0.05807\n",
      "Epoch: 41, loss: 0.04312\n",
      "Epoch: 42, loss: 0.05171\n",
      "Epoch: 43, loss: 0.04146\n",
      "Epoch: 44, loss: 0.05367\n",
      "Epoch: 45, loss: 0.04455\n",
      "Epoch: 46, loss: 0.04034\n",
      "Epoch: 47, loss: 0.03652\n",
      "Epoch: 48, loss: 0.03735\n",
      "Epoch: 49, loss: 0.04982\n",
      "Epoch: 50, loss: 0.04436\n",
      "Epoch: 51, loss: 0.03781\n",
      "Epoch: 52, loss: 0.04714\n",
      "Epoch: 53, loss: 0.05201\n",
      "Epoch: 54, loss: 0.04762\n",
      "Epoch: 55, loss: 0.04460\n",
      "Epoch: 56, loss: 0.06033\n",
      "Epoch: 57, loss: 0.04728\n",
      "Epoch: 58, loss: 0.04550\n",
      "Epoch: 59, loss: 0.04026\n",
      "Epoch: 60, loss: 0.03887\n",
      "Epoch: 61, loss: 0.03434\n",
      "Epoch: 62, loss: 0.04107\n",
      "Epoch: 63, loss: 0.03079\n",
      "Epoch: 64, loss: 0.02930\n",
      "Epoch: 65, loss: 0.03101\n",
      "Epoch: 66, loss: 0.03127\n",
      "Epoch: 67, loss: 0.03739\n",
      "Epoch: 68, loss: 0.04148\n",
      "Epoch: 69, loss: 0.03850\n",
      "Epoch: 70, loss: 0.02957\n",
      "Epoch: 71, loss: 0.02992\n",
      "Epoch: 72, loss: 0.02822\n",
      "Epoch: 73, loss: 0.04019\n",
      "Epoch: 74, loss: 0.03464\n",
      "Epoch: 75, loss: 0.02919\n",
      "Epoch: 76, loss: 0.02810\n",
      "Epoch: 77, loss: 0.03352\n",
      "Epoch: 78, loss: 0.03117\n",
      "Epoch: 79, loss: 0.03277\n",
      "Epoch: 80, loss: 0.04075\n",
      "Epoch: 81, loss: 0.02828\n",
      "Epoch: 82, loss: 0.03023\n",
      "Epoch: 83, loss: 0.02998\n",
      "Epoch: 84, loss: 0.03135\n",
      "Epoch: 85, loss: 0.03112\n",
      "Epoch: 86, loss: 0.02686\n",
      "Epoch: 87, loss: 0.03305\n",
      "Epoch: 88, loss: 0.03248\n",
      "Epoch: 89, loss: 0.03613\n",
      "Epoch: 90, loss: 0.04335\n",
      "Epoch: 91, loss: 0.03294\n",
      "Epoch: 92, loss: 0.03699\n",
      "Epoch: 93, loss: 0.02885\n",
      "Epoch: 94, loss: 0.03442\n",
      "Epoch: 95, loss: 0.03406\n",
      "Epoch: 96, loss: 0.02545\n",
      "Epoch: 97, loss: 0.02940\n",
      "Epoch: 98, loss: 0.03332\n",
      "Epoch: 99, loss: 0.02427\n",
      "Epoch: 100, loss: 0.04028\n",
      "Epoch: 101, loss: 0.03195\n",
      "Epoch: 102, loss: 0.03162\n",
      "Epoch: 103, loss: 0.02870\n",
      "Epoch: 104, loss: 0.01946\n",
      "Epoch: 105, loss: 0.03406\n",
      "Epoch: 106, loss: 0.03124\n",
      "Epoch: 107, loss: 0.02974\n",
      "Epoch: 108, loss: 0.02967\n",
      "Epoch: 109, loss: 0.02525\n",
      "Epoch: 110, loss: 0.03230\n",
      "Epoch: 111, loss: 0.02482\n",
      "Epoch: 112, loss: 0.03378\n",
      "Epoch: 113, loss: 0.01941\n",
      "Epoch: 114, loss: 0.02907\n",
      "Epoch: 115, loss: 0.02202\n",
      "Epoch: 116, loss: 0.02188\n",
      "Epoch: 117, loss: 0.02018\n",
      "Epoch: 118, loss: 0.02624\n",
      "Epoch: 119, loss: 0.03451\n",
      "Epoch: 120, loss: 0.02005\n",
      "Epoch: 121, loss: 0.02282\n",
      "Epoch: 122, loss: 0.02180\n",
      "Epoch: 123, loss: 0.01961\n",
      "Epoch: 124, loss: 0.01455\n",
      "Epoch: 125, loss: 0.01797\n",
      "Epoch: 126, loss: 0.02330\n",
      "Epoch: 127, loss: 0.02018\n",
      "Epoch: 128, loss: 0.02249\n",
      "Epoch: 129, loss: 0.02085\n",
      "Epoch: 130, loss: 0.02741\n",
      "Epoch: 131, loss: 0.02048\n",
      "Epoch: 132, loss: 0.01960\n",
      "Epoch: 133, loss: 0.02268\n",
      "Epoch: 134, loss: 0.02238\n",
      "Epoch: 135, loss: 0.02173\n",
      "Epoch: 136, loss: 0.02301\n",
      "Epoch: 137, loss: 0.02129\n",
      "Epoch: 138, loss: 0.02194\n",
      "Epoch: 139, loss: 0.01880\n",
      "Epoch: 140, loss: 0.02363\n",
      "Epoch: 141, loss: 0.01956\n",
      "Epoch: 142, loss: 0.02610\n",
      "Epoch: 143, loss: 0.02781\n",
      "Epoch: 144, loss: 0.02859\n",
      "Epoch: 145, loss: 0.01633\n",
      "Epoch: 146, loss: 0.02039\n",
      "Epoch: 147, loss: 0.01924\n",
      "Epoch: 148, loss: 0.02591\n",
      "Epoch: 149, loss: 0.02892\n",
      "Epoch: 150, loss: 0.01664\n",
      "Epoch: 151, loss: 0.02307\n",
      "Epoch: 152, loss: 0.03162\n",
      "Epoch: 153, loss: 0.01739\n",
      "Epoch: 154, loss: 0.02502\n",
      "Epoch: 155, loss: 0.01309\n",
      "Epoch: 156, loss: 0.02240\n",
      "Epoch: 157, loss: 0.03048\n",
      "Epoch: 158, loss: 0.01865\n",
      "Epoch: 159, loss: 0.01759\n",
      "Epoch: 160, loss: 0.01962\n",
      "Epoch: 161, loss: 0.02337\n",
      "Epoch: 162, loss: 0.02199\n",
      "Epoch: 163, loss: 0.03196\n",
      "Epoch: 164, loss: 0.02436\n",
      "Epoch: 165, loss: 0.03069\n",
      "Epoch: 166, loss: 0.02272\n",
      "Epoch: 167, loss: 0.02074\n",
      "Epoch: 168, loss: 0.01932\n",
      "Epoch: 169, loss: 0.02743\n",
      "Epoch: 170, loss: 0.02152\n",
      "Epoch: 171, loss: 0.01757\n",
      "Epoch: 172, loss: 0.01766\n",
      "Epoch: 173, loss: 0.02991\n",
      "Epoch: 174, loss: 0.02281\n",
      "Epoch: 175, loss: 0.01842\n",
      "Epoch: 176, loss: 0.02467\n",
      "Epoch: 177, loss: 0.01964\n",
      "Epoch: 178, loss: 0.02445\n",
      "Epoch: 179, loss: 0.01748\n",
      "Epoch: 180, loss: 0.02050\n",
      "Epoch: 181, loss: 0.02098\n",
      "Epoch: 182, loss: 0.01929\n",
      "Epoch: 183, loss: 0.01695\n",
      "Epoch: 184, loss: 0.01885\n",
      "Epoch: 185, loss: 0.02679\n",
      "Epoch: 186, loss: 0.02045\n",
      "Epoch: 187, loss: 0.01675\n",
      "Epoch: 188, loss: 0.02088\n",
      "Epoch: 189, loss: 0.01542\n",
      "Epoch: 190, loss: 0.01687\n",
      "Epoch: 191, loss: 0.02102\n",
      "Epoch: 192, loss: 0.02108\n",
      "Epoch: 193, loss: 0.02012\n",
      "Epoch: 194, loss: 0.01880\n",
      "Epoch: 195, loss: 0.02127\n",
      "Epoch: 196, loss: 0.01708\n",
      "Epoch: 197, loss: 0.02235\n",
      "Epoch: 198, loss: 0.01508\n",
      "Epoch: 199, loss: 0.02123\n"
     ]
    }
   ],
   "source": [
    "ae_data = get_data_between_time(\n",
    "    data = data,\n",
    "    from_date = date(month=1, year=2005, day=1),\n",
    "    to_date = date(month=12, year=2005, day=31),\n",
    "    dt_idx=1\n",
    ")\n",
    "\n",
    "ae_data = Variable(torch.from_numpy(ae_data[:, 3:].astype(np.float32)))\n",
    "\n",
    "autoencoder = Autoencoder(\n",
    "    input_size = 25,\n",
    "    down_size = 15\n",
    ")\n",
    "\n",
    "losses = autoencoder.train(\n",
    "    data = ae_data, \n",
    "    batch_size = 20, \n",
    "    epochs = 200, \n",
    "    lr = 0.000005,\n",
    "    no_act = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(autoencoder.state_dict(), './data/autoencoder_params')\n",
    "\n",
    "# new_autoencoder = Autoencoder(\n",
    "#     input_size = 25,\n",
    "#     down_size = 15\n",
    "# )\n",
    "\n",
    "# new_autoencoder.load_state_dict(torch.load('./data/autoencoder_params'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Autoencoder Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Reduced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTensor = Variable(torch.from_numpy(data[:, 3:].astype(np.float32)))\n",
    "encoded_data_without_date = autoencoder.forward(dataTensor, decode=False, no_act=True).detach().numpy()\n",
    "encoded_data = np.concatenate((data[:, :3], encoded_data_without_date), axis=1)\n",
    "embedded_encoded_data, target = embed_data(\n",
    "    lead_time = 6,\n",
    "    embed_dim = 12,\n",
    "    from_date = date(month=1, year=2006, day=1),\n",
    "    to_date = date(month=3, year=2006, day=31),\n",
    "    data = encoded_data,\n",
    "    t_idx = 2,\n",
    "    dt_idx = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM for forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(file_name):\n",
    "    data = pd.read_csv(file_name)\n",
    "    data = data.to_numpy()\n",
    "    for i in range(data.shape[0]):\n",
    "        data[i, 1] = datetime.strptime(data[i, 1], \"%m%d%Y %H:%M\").date()\n",
    "    power = data[:, 2].astype(np.float64)\n",
    "    indexes = np.isnan(power)\n",
    "    cleaned_data = data[~indexes, :]\n",
    "    return cleaned_data\n",
    "\n",
    "def embed_data(lead_time, embed_dim, from_date, to_date, data, t_idx, dt_idx):\n",
    "    start = np.where(data[:, dt_idx] == from_date)[0][0]\n",
    "    end = np.where(data[:, dt_idx] == to_date)[0][-1]\n",
    "    X, y = [], []\n",
    "    for idx in range(start, end+1):\n",
    "        xx = data[idx-(embed_dim+lead_time-1):idx-lead_time+1, 3:]\n",
    "        yy = data[idx, 2]\n",
    "        X.append(xx)\n",
    "        y.append(yy)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def get_data_between_time(data, from_date, to_date, dt_idx):\n",
    "    start = np.where(data[:, dt_idx] == from_date)[0][0]\n",
    "    end = np.where(data[:, dt_idx] == to_date)[0][-1]\n",
    "    return data[start:(end+1), :]\n",
    "\n",
    "\n",
    "def normalize(data):\n",
    "    for i in range(2, data.shape[1]):\n",
    "        data[:, i] = (data[:, i] - np.mean(data[:, i]))/np.std(data[:, i])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data('./data/task_1/L1-train.csv')\n",
    "data = normalize(data)\n",
    "embedded_data, target = embed_data(\n",
    "    lead_time = 6,\n",
    "    embed_dim = 12,\n",
    "    from_date = date(month=2, year=2005, day=1),\n",
    "    to_date = date(month=4, year=2005, day=30),\n",
    "    data = data,\n",
    "    t_idx = 2,\n",
    "    dt_idx = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantileRegression(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size, num_layers, seq_length):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_length = seq_length\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=input_size, \n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, \n",
    "                            batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, 10)\n",
    "        self.fc_out = nn.Linear(10, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_0 = (Variable(torch.zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_size)),Variable(torch.zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_size))) \n",
    "        \n",
    "        c_0 = (Variable(torch.zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_size)),Variable(torch.zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_size)))\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "        # Propagate input through LSTM\n",
    "        ula, (h_out, _) = self.lstm(x, (h_0, c_0))\n",
    "        h_out = h_out.view(batch_size*self.num_layers, self.hidden_size)\n",
    "        \n",
    "        fc1 = self.fc(h_out)\n",
    "        out = self.fc_out(fc1)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def train_model(self, trainX, trainY, lr, epochs, sample_size, alpha):\n",
    "        learning_rate = lr\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        losses = []\n",
    "        num_epochs = epochs\n",
    "        \n",
    "        for epoch in range(num_epochs):  \n",
    "            indices = torch.randperm(trainX.size()[0])\n",
    "            trainX, trainY = trainX[indices, :, :], trainY[indices]\n",
    "            for i in range(0, trainX.size()[0], sample_size):\n",
    "                xx = trainX[i: i + sample_size, :, :]\n",
    "                yy = trainY[i: i + sample_size]\n",
    "                outputs = self.forward(xx)\n",
    "                outputs = outputs.view(self.num_layers, self.output_size, -1)\n",
    "                optimizer.zero_grad()\n",
    "                # obtain the loss function\n",
    "                loss = self.quantile_loss(outputs[-1, :, :], yy, alpha)\n",
    "                losses.append(loss.item())\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "            print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item()))\n",
    "            \n",
    "        return losses\n",
    "    \n",
    "    def train_sgd(self, trainX, trainY, lr, epochs, sample_size):\n",
    "        learning_rate = lr\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        losses = []\n",
    "        num_epochs = epochs\n",
    "        for epoch in range(num_epochs):\n",
    "            for xx, yy in zip(trainX, trainY):\n",
    "                xx = xx.view(1, self.seq_length, self.input_size)\n",
    "                outputs = self.forward(xx)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # obtain the loss function\n",
    "                loss = self.quantile_loss(outputs, yy, alpha=0.9)\n",
    "                losses.append(loss)\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "            print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item()))\n",
    "        return losses\n",
    "    \n",
    "    def quantile_loss(self, output, target, alpha = 0.5):\n",
    "        covered_flag = (output <= target).float()\n",
    "        uncovered_flag = (output > target).float()\n",
    "        return torch.mean((target - output)*(alpha)*covered_flag + (output-target)*(1-alpha)*uncovered_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(25, 60, batch_first=True, bidirectional=True)\n",
      "(2136, 12, 25)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected hidden[0] size (2, 1, 60), got (1, 1, 60)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-2bfaaf5f7d09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mxx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myy\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mxx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-2bfaaf5f7d09>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mh0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mc0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mrnnOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0mhn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayerNum\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatchSize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhiddenNum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mfcOutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/course_manager/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/course_manager/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_packed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/course_manager/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_tensor\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0munsorted_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munsorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/course_manager/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_impl\u001b[0;34m(self, input, hx, batch_sizes, max_batch_size, sorted_indices)\u001b[0m\n\u001b[1;32m    521\u001b[0m             \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m             result = _VF.lstm(input, hx, self._get_flat_weights(), self.bias, self.num_layers,\n",
      "\u001b[0;32m~/anaconda3/envs/course_manager/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m         self.check_hidden_size(hidden[0], expected_hidden_size,\n\u001b[0;32m--> 500\u001b[0;31m                                'Expected hidden[0] size {}, got {}')\n\u001b[0m\u001b[1;32m    501\u001b[0m         self.check_hidden_size(hidden[1], expected_hidden_size,\n\u001b[1;32m    502\u001b[0m                                'Expected hidden[1] size {}, got {}')\n",
      "\u001b[0;32m~/anaconda3/envs/course_manager/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_hidden_size\u001b[0;34m(self, hx, expected_hidden_size, msg)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;31m# type: (Tensor, Tuple[int, int, int], str) -> None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected hidden[0] size (2, 1, 60), got (1, 1, 60)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "\n",
    "# RNNs模型基类，主要是用于指定参数和cell类型\n",
    "class BaseModel(nn.Module):\n",
    "\n",
    "    def __init__(self, inputDim, hiddenNum, outputDim, layerNum, cell, use_cuda=False):\n",
    "\n",
    "        super(BaseModel, self).__init__()\n",
    "        self.hiddenNum = hiddenNum\n",
    "        self.inputDim = inputDim\n",
    "        self.outputDim = outputDim\n",
    "        self.layerNum = layerNum\n",
    "        self.use_cuda = use_cuda\n",
    "        if cell == \"RNN\":\n",
    "            self.cell = nn.RNN(input_size=self.inputDim, hidden_size=self.hiddenNum,\n",
    "                        num_layers=self.layerNum, dropout=0.0,\n",
    "                         nonlinearity=\"tanh\", batch_first=True,)\n",
    "        if cell == \"LSTM\":\n",
    "            self.cell = nn.LSTM(input_size=self.inputDim, hidden_size=self.hiddenNum,\n",
    "                               num_layers=self.layerNum, dropout=0.0,\n",
    "                               batch_first=True, bidirectional=True)\n",
    "        if cell == \"GRU\":\n",
    "            self.cell = nn.GRU(input_size=self.inputDim, hidden_size=self.hiddenNum,\n",
    "                                num_layers=self.layerNum, dropout=0.0,\n",
    "                                 batch_first=True, )\n",
    "        print(self.cell)\n",
    "        self.fc = nn.Linear(self.hiddenNum, self.outputDim)\n",
    "\n",
    "\n",
    "# 标准RNN模型\n",
    "class RNNModel(BaseModel):\n",
    "\n",
    "    def __init__(self, inputDim, hiddenNum, outputDim, layerNum, cell, use_cuda):\n",
    "\n",
    "        super(RNNModel, self).__init__(inputDim, hiddenNum, outputDim, layerNum, cell, use_cuda)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        batchSize = x.size(0)\n",
    "        h0 = Variable(torch.zeros(self.layerNum * 1, batchSize , self.hiddenNum))\n",
    "        if self.use_cuda:\n",
    "            h0 = h0.cuda()\n",
    "        rnnOutput, hn = self.cell(x, h0)\n",
    "        hn = hn.view(batchSize, self.hiddenNum)\n",
    "        fcOutput = self.fc(hn)\n",
    "\n",
    "        return fcOutput\n",
    "\n",
    "\n",
    "# LSTM模型\n",
    "class LSTMModel(BaseModel):\n",
    "\n",
    "    def __init__(self, inputDim, hiddenNum, outputDim, layerNum, cell, use_cuda):\n",
    "        super(LSTMModel, self).__init__(inputDim, hiddenNum, outputDim, layerNum, cell, use_cuda)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchSize = x.size(0)\n",
    "        h0 = Variable(torch.zeros(self.layerNum * 1, batchSize, self.hiddenNum))\n",
    "        c0 = Variable(torch.zeros(self.layerNum * 1, batchSize, self.hiddenNum))\n",
    "        if self.use_cuda:\n",
    "            h0 = h0.cuda()\n",
    "            c0 = c0.cuda()\n",
    "        rnnOutput, hn = self.cell(x, (h0, c0))\n",
    "        hn = hn[0].view(self.layerNum * batchSize, self.hiddenNum)\n",
    "        fcOutput = self.fc(hn)\n",
    "\n",
    "        return fcOutput\n",
    "    \n",
    "    \n",
    "lstm_model = LSTMModel(\n",
    "    inputDim = 25,\n",
    "    hiddenNum = 60,\n",
    "    outputDim = 1,\n",
    "    layerNum = 1,\n",
    "    cell = \"LSTM\",\n",
    "    use_cuda = False\n",
    ")\n",
    "\n",
    "def quantile_loss(output, target, alpha = 0.5):\n",
    "#     print(output.size(), target.size())\n",
    "    covered_flag = (output <= target).float()\n",
    "    uncovered_flag = (output > target).float()\n",
    "    return torch.mean((target - output)*(alpha)*covered_flag + (output-target)*(1-alpha)*uncovered_flag)\n",
    "\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=learning_rate)\n",
    "losses = []\n",
    "num_epochs = 1\n",
    "print(embedded_data.shape)\n",
    "criterion = torch.nn.MSELoss()\n",
    "train_x = embedded_data.astype(np.float32)\n",
    "train_y = target.astype(np.float32)\n",
    "\n",
    "trainX = Variable(torch.from_numpy(train_x))\n",
    "trainY = Variable(torch.from_numpy(train_y))\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for xx, yy in zip(trainX, trainY):\n",
    "        xx = xx.view(1, 12, 25)\n",
    "        outputs = lstm_model.forward(xx)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # obtain the loss function\n",
    "        loss = quantile_loss(outputs, yy, alpha=0.9)\n",
    "        losses.append(loss)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "    print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-b92647d663af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0msample_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     )\n\u001b[1;32m     26\u001b[0m     \u001b[0mregressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-87bcacd20294>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, trainX, trainY, lr, epochs, sample_size, alpha)\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0mxx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0myy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-87bcacd20294>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Propagate input through LSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mula\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mh_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/course_manager/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/course_manager/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_packed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/course_manager/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_tensor\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0munsorted_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munsorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/course_manager/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_impl\u001b[0;34m(self, input, hx, batch_sizes, max_batch_size, sorted_indices)\u001b[0m\n\u001b[1;32m    521\u001b[0m             \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m             result = _VF.lstm(input, hx, self._get_flat_weights(), self.bias, self.num_layers,\n",
      "\u001b[0;32m~/anaconda3/envs/course_manager/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m         self.check_hidden_size(hidden[0], expected_hidden_size,\n\u001b[0;32m--> 500\u001b[0;31m                                'Expected hidden[0] size {}, got {}')\n\u001b[0m\u001b[1;32m    501\u001b[0m         self.check_hidden_size(hidden[1], expected_hidden_size,\n\u001b[1;32m    502\u001b[0m                                'Expected hidden[1] size {}, got {}')\n",
      "\u001b[0;32m~/anaconda3/envs/course_manager/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_hidden_size\u001b[0;34m(self, hx, expected_hidden_size, msg)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_hidden_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Expected hidden size {}, got {}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;31m# type: (Tensor, Tuple[int, int, int], str) -> None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "regressor = QuantileRegression(\n",
    "        input_size = 25,\n",
    "        hidden_size = 40,\n",
    "        output_size = 1,\n",
    "        num_layers = 1,\n",
    "        seq_length = 12\n",
    "    )\n",
    "\n",
    "train_x = embedded_data.astype(np.float32)\n",
    "train_y = target.astype(np.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "alpha_list = [0.9]\n",
    "for alpha in alpha_list:\n",
    "    losses = regressor.train_model(\n",
    "        trainX = trainX,\n",
    "        trainY = trainY,\n",
    "        lr = 0.005,\n",
    "        epochs = 200,\n",
    "        sample_size = 50,\n",
    "        alpha=alpha\n",
    "    )\n",
    "    regressor.eval()\n",
    "    train_predict = regressor(trainX)\n",
    "    data_predict = train_predict.data.numpy()\n",
    "    dataY_plot =  trainY.data.numpy()\n",
    "    label_string = \"alpha = %f\"%(alpha)\n",
    "    print(data_predict.shape, dataY_plot.shape)    \n",
    "    plt.plot(data_predict[:200], '--' ,label=label_string)\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "trainX = Variable(torch.from_numpy(train_x))\n",
    "trainY = Variable(torch.from_numpy(train_y))\n",
    "figure(num=None, figsize=(20, 8), dpi=80, facecolor='w', edgecolor='k')\n",
    "layers  = 1\n",
    "from_here = int(data_predict.shape[0] / layers)\n",
    "plt.plot(dataY_plot[:200], label=\"Original Y data\")\n",
    "plt.xlabel('Timesteps')\n",
    "plt.ylabel('Electric Load')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
