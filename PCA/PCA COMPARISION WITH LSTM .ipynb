{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder\n",
    "\n",
    "This is a 4 layers Autoencoder(2 encoder + 2 decoder) for performing dimensionality reduction on the GEFCOM-2014 load forecasting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date, timedelta, datetime\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from matplotlib.pyplot import figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing & Staging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(file_name):\n",
    "    data = pd.read_csv(file_name)\n",
    "    data = data.to_numpy()\n",
    "    for i in range(data.shape[0]):\n",
    "        data[i, 1] = datetime.strptime(data[i, 1], \"%m%d%Y %H:%M\").date()\n",
    "    power = data[:, 2].astype(np.float64)\n",
    "    indexes = np.isnan(power)\n",
    "    cleaned_data = data[~indexes, :]\n",
    "    return cleaned_data\n",
    "\n",
    "def embed_data(lead_time, embed_dim, from_date, to_date, data, t_idx, dt_idx):\n",
    "    start = np.where(data[:, dt_idx] == from_date)[0][0]\n",
    "    end = np.where(data[:, dt_idx] == to_date)[0][-1]\n",
    "    X, y = [], []\n",
    "    for idx in range(start, end+1):\n",
    "        xx = data[idx-(embed_dim+lead_time-1):idx-lead_time+1, 3:]\n",
    "        yy = data[idx, 2]\n",
    "        X.append(xx)\n",
    "        y.append(yy)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def get_data_between_time(data, from_date, to_date, dt_idx):\n",
    "    start = np.where(data[:, dt_idx] == from_date)[0][0]\n",
    "    end = np.where(data[:, dt_idx] == to_date)[0][-1]\n",
    "    return data[start:(end+1), :]\n",
    "\n",
    "\n",
    "def normalize(data):\n",
    "    for i in range(2, data.shape[1]):\n",
    "        data[:, i] = (data[:, i] - np.mean(data[:, i]))/np.std(data[:, i])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "data = get_data('../Dataa/Task 1/L1-train.csv')\n",
    "data = normalize(data)\n",
    "dataactual=data[:,3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50376, 25)\n"
     ]
    }
   ],
   "source": [
    "print(dataactual.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.0682467564243037 -0.9319257986324055 -0.9618264508933926 ...\n",
      "  -0.8900772206990581 -0.8460325215149895 -0.9108400548369758]\n",
      " [-1.1831429734183943 -0.9319257986324055 -1.0832982359183199 ...\n",
      "  -0.8900772206990581 -0.8460325215149895 -0.9108400548369758]\n",
      " [-1.2405910819154395 -0.9319257986324055 -1.1440341284307833 ...\n",
      "  -0.9475564729698909 -0.9038780725289747 -1.0265962403074795]\n",
      " ...\n",
      " [0.7126446069841011 0.47318180138723054 0.3136272918683433 ...\n",
      "  0.48942483380092805 0.4844151518066707 0.5361122635443207]\n",
      " [0.6551964984870559 0.47318180138723054 0.3136272918683433 ...\n",
      "  0.5469040860717608 0.6001062538346411 0.5361122635443207]\n",
      " [0.5403002814929652 0.47318180138723054 0.3743631843808069 ...\n",
      "  0.5469040860717608 0.36872404977870027 0.4782341708090689]]\n"
     ]
    }
   ],
   "source": [
    "print(dataactual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcaa = PCA(n_components=10)\n",
    "principalComponents = pcaa.fit_transform(dataactual)\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['1','2','3','4','5','6','7','8','9','10'])\n",
    "pdf=principalDf.to_numpy()\n",
    "result = np.hstack(( np.atleast_2d(data[:,2]).T,pdf)) \n",
    "result = np.hstack(( np.atleast_2d(data[:,1]).T,result)) \n",
    "result = np.hstack((np.atleast_2d(data[:,0]).T,result)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50375\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.566602</td>\n",
       "      <td>-0.093160</td>\n",
       "      <td>-0.343859</td>\n",
       "      <td>0.067665</td>\n",
       "      <td>0.029243</td>\n",
       "      <td>-0.442420</td>\n",
       "      <td>0.018236</td>\n",
       "      <td>-0.174663</td>\n",
       "      <td>0.151819</td>\n",
       "      <td>-0.126880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.628722</td>\n",
       "      <td>0.004835</td>\n",
       "      <td>-0.272167</td>\n",
       "      <td>-0.098046</td>\n",
       "      <td>-0.011761</td>\n",
       "      <td>-0.315610</td>\n",
       "      <td>0.197051</td>\n",
       "      <td>-0.174687</td>\n",
       "      <td>0.036394</td>\n",
       "      <td>-0.023859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.910206</td>\n",
       "      <td>-0.049068</td>\n",
       "      <td>-0.450476</td>\n",
       "      <td>-0.149179</td>\n",
       "      <td>0.039220</td>\n",
       "      <td>-0.252723</td>\n",
       "      <td>0.211010</td>\n",
       "      <td>-0.124321</td>\n",
       "      <td>-0.062594</td>\n",
       "      <td>0.070434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.861920</td>\n",
       "      <td>-0.102269</td>\n",
       "      <td>-0.307686</td>\n",
       "      <td>-0.155568</td>\n",
       "      <td>-0.060778</td>\n",
       "      <td>-0.309647</td>\n",
       "      <td>0.221295</td>\n",
       "      <td>-0.045340</td>\n",
       "      <td>0.180990</td>\n",
       "      <td>0.097413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.996918</td>\n",
       "      <td>-0.095231</td>\n",
       "      <td>-0.381688</td>\n",
       "      <td>-0.262827</td>\n",
       "      <td>0.008140</td>\n",
       "      <td>-0.278533</td>\n",
       "      <td>0.167702</td>\n",
       "      <td>-0.081630</td>\n",
       "      <td>0.199273</td>\n",
       "      <td>0.091227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          1         2         3         4         5         6         7  \\\n",
       "0  4.566602 -0.093160 -0.343859  0.067665  0.029243 -0.442420  0.018236   \n",
       "1  4.628722  0.004835 -0.272167 -0.098046 -0.011761 -0.315610  0.197051   \n",
       "2  4.910206 -0.049068 -0.450476 -0.149179  0.039220 -0.252723  0.211010   \n",
       "3  4.861920 -0.102269 -0.307686 -0.155568 -0.060778 -0.309647  0.221295   \n",
       "4  4.996918 -0.095231 -0.381688 -0.262827  0.008140 -0.278533  0.167702   \n",
       "\n",
       "          8         9        10  \n",
       "0 -0.174663  0.151819 -0.126880  \n",
       "1 -0.174687  0.036394 -0.023859  \n",
       "2 -0.124321 -0.062594  0.070434  \n",
       "3 -0.045340  0.180990  0.097413  \n",
       "4 -0.081630  0.199273  0.091227  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(principalDf.index[0])\n",
    "print(principalDf.index[-1])\n",
    "principalDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50376, 13)\n",
      "[[1 datetime.date(2005, 1, 1) -0.4252401471841657 ...\n",
      "  -0.17466340257377716 0.15181939416924545 -0.12688010056566054]\n",
      " [1 datetime.date(2005, 1, 1) -0.5109688790207725 ...\n",
      "  -0.17468743781775703 0.03639439868870091 -0.023858938180635437]\n",
      " [1 datetime.date(2005, 1, 1) -0.6138433572247005 ...\n",
      "  -0.12432091681162749 -0.06259422904717793 0.07043385557728205]\n",
      " ...\n",
      " [1 datetime.date(2010, 9, 30) 0.2177253415903853 ...\n",
      "  -0.010984092620579573 -0.16075311855581184 0.1178053653482231]\n",
      " [1 datetime.date(2010, 9, 30) -0.03946085391943498 ...\n",
      "  0.08605060434295038 -0.23467152825507911 0.04040907201163119]\n",
      " [1 datetime.date(2010, 10, 1) -0.33093854216389845 ...\n",
      "  -0.06301255846719225 -0.20934241515206659 -0.027374703059433236]]\n"
     ]
    }
   ],
   "source": [
    "print(result.shape)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_data_pca, target_pca = embed_data(\n",
    "    lead_time = 6,\n",
    "    embed_dim = 12,\n",
    "    from_date = date(month=2, year=2006, day=1),\n",
    "    to_date = date(month=4, year=2006, day=30),\n",
    "    data = result,\n",
    "    t_idx = 2,\n",
    "    dt_idx = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_data_original, target_original = embed_data(\n",
    "    lead_time = 6,\n",
    "    embed_dim = 12,\n",
    "    from_date = date(month=2, year=2006, day=1),\n",
    "    to_date = date(month=4, year=2006, day=30),\n",
    "    data = data,\n",
    "    t_idx = 2,\n",
    "    dt_idx = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(autoencoder.state_dict(), './data/autoencoder_params')\n",
    "\n",
    "# new_autoencoder = Autoencoder(\n",
    "#     input_size = 25,\n",
    "#     down_size = 15\n",
    "# )\n",
    "\n",
    "# new_autoencoder.load_state_dict(torch.load('./data/autoencoder_params'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3de5xdZX3v8c83mUxuJExCAuQG4RJFtBJwCkGtRdAQgWOoRzBUJSI1R489rdpzNKgtikWhF0VrxaYgopWbqCWlCEYuWquJTJCLEDBDwGRISCbkTshlJr/zx3oGVpI9s/dc9mVmvu/Xa7/2Ws9aa+/fXmzmm2ettZ+liMDMzKwrQ6pdgJmZ1T6HhZmZFeWwMDOzohwWZmZWlMPCzMyKcliYmVlRDguzGiMpJB1f7TrM8hwW1i9IekDSZknDu7ndoP7DK+kDkn5R7Tqs/3NYWM2TNB34IyCAd1a1mBomqa7aNdjA5bCw/uBiYCnwbWB+fkHqcfxZbv7lf0lL+nlqfkTSDknvSe0fktQsaZOkxZIm57Y/QdKStOwpSRfmln1b0j9L+k9J2yUtk3Rcbvlrc9uul/Tp1D5c0jWS1qbHNfkekqT/J2ldWvbBAz7fcEn/IGl1es1vShqZlp0hqUXSpyQ9D9zQnZ0qaXL6/JvS/vhQbtmpkpokbUvv++XUPkLSv0l6QdIWSQ9KOqI772v9k8PC+oOLge+lx9ml/nGKiLekyZMi4pCIuFXSmcCXgAuBScDvgVsAJI0GlgA3AYcDFwHfkPTa3MteBHweGAc0A1embccAPwXuBiYDxwP3pm0+A8wCZgInAacCn03bzQH+L/B2YAbwtgM+xtXAq9K2xwNTgL/JLT8SGA8cDSwoZb/k3Ay0pHrfDXxR0llp2VeBr0bEWOA44LbUPh84FJgGHAZ8GHipm+9r/ZDDwmqapDeT/SG8LSKWA08Df9qLl3wv8K2IeCgidgOXAaenQ13nAc9GxA0R0RYRDwE/IPtD2uGHEfHriGgjC6+Zqf084PmI+MeI2BUR2yNiWe49r4iIDRHRShY270/LLgRuiIjfRsSLwOdyn13Ah4CPR8SmiNgOfBGYl6tnH3B5ROyOiJL/aEuaBrwZ+FSq92Hgulxde4HjJU2IiB0RsTTXfhhwfES0R8TyiNhW6vta/+WwsFo3H/hJRGxM8zdxwKGobppM1psAICJ2AC+Q/Yv9aOC0dHhli6QtZH/oj8xt/3xueidwSJqeRhZkRd8zTU/OLVtzwLIOE4FRwPJcPXen9g6tEbGrk/ftymSgI4Dy7z0lTV9K1qN5Mh1qOi+1fxe4B7glHTb7O0nDevD+1s/4hJjVrHRs/kJgaDomDzAcaJB0UkQ8ArxI9ge1w5F0bS1ZKHS8x2iyfyk/R/ZH+2cR8fYelLuG7BBVV+/5eJo/KrUBrCMLGnLLOmwkO8Tz2oh4rpPX7umw0WuB8ZLG5ALjKLL9QESsBC6SNAR4F3C7pMNS7+fzwOdTb+wu4Cng+h7WYf2EexZWy84H2oETyQ73zAReA/wX2XkMgIeBd0kalS6RvfSA11gPHJubvwm4RNLMdJL5i8CyiHgWuBN4laT3SxqWHn8o6TUl1HoncKSkj6WT0mMknZaW3Qx8VtJESRPIzjn8W1p2G/ABSSdKGgVc3vGCEbEP+FfgK5IOB5A0RdLZJdSTp3Ri+uVHRKwBfgl8KbW9nmzffS9t8D5JE1MNW9LrtEt6q6Q/kDQU2EZ2WKq9m/VYP+SwsFo2n+x4/uqIeL7jAXwdeK+yS0W/AuwhC4UbSX/scj4H3JgO41wYEfcCf012LmId2cnbeQDpX9iz0/xaskNOV5P1ZrqUtn078D/SdiuBt6bFfws0AY8CjwEPpTYi4sfANcB9ZCfM7zvgpT+V2pdK2kZ2Ev3Vxeo5wBvJeigvP9K+uwiYnj7rj8jOfSxJ28wBHpe0g+xk97x0uOtI4HayoFgB/IxXgs8GMPnmR2ZmVox7FmZmVpTDwszMinJYmJlZUQ4LMzMrakD+zmLChAkxffr0apdhZtavLF++fGNETCy0rKxhIamBbAiB15H9eOiDZD/guZXskr1ngQsjYnMa2uCrwDlkv4z9QBpuAUnzSWPpAH8bETd29b7Tp0+nqampzz+PmdlAJun3nS0r92GorwJ3R8QJZAOorQAWAvdGxAyygdYWpnXfQTaQ2gyyAdGuBZA0nuyHSqeRDcB2uaRxZa7bzMxyyhYWksYCbyENAxAReyJiCzCX7MdTpOfz0/Rc4DuRWUo2pMMk4GxgSRpIbTPZqKBzylW3mZkdrJw9i2OBVuAGSb+RdF0ah+eIiFgHkJ4PT+tPYf8B1VpSW2ft+5G0II2/39Ta2tr3n8bMbBArZ1jUAacA10bEyWQDvi3sYn0VaIsu2vdviFgUEY0R0ThxYsHzM2Zm1kPlDIsWoCU3pv/tZOGxPh1eIj1vyK2fH31zKtmYNZ21m5lZhZQtLNKAb2skdQx6dhbwBLCYV+5HMB+4I00vBi5WZhawNR2mugeYLWlcOrE9O7WZmVmFlPt3Fv8H+J6kemAVcAlZQN0m6VJgNXBBWvcusstmm8kunb0EICI2SfoC8GBa74qI2FTmus3MLGdAjjrb2NgYPf2dxarWHTy/bRdvPG5CH1dlZlbbJC2PiMZCywbkL7h748x//BkAz151bpUrMTOrHR4byszMinJYmJlZUQ4LMzMrymFhZmZFOSzMzKwoh4WZmRXlsDAzs6IcFp3Ytbe92iWYmdUMh0UnTvjru6tdgplZzXBYmJlZUQ4LMzMrymFhZmZFOSzMzKwoh4WZmRXlsDAzs6IcFmZmVpTDogsD8S6CZmY94bDowr8//Fy1SzAzqwkOi5wnn9+23/yWnXurVImZWW1xWOQ8u/HFapdgZlaTHBY5zRt27Df/3OaXeOOX7mXtlpeqVJGZWW1wWOTsadu33/x1v3iGtVt38aPf+NyFmQ1uDgszMyvKYZHjC2XNzAora1hIelbSY5IeltSU2sZLWiJpZXoel9ol6WuSmiU9KumU3OvMT+uvlDS/XPX6ZxVmZoVVomfx1oiYGRGNaX4hcG9EzADuTfMA7wBmpMcC4FrIwgW4HDgNOBW4vCNgzMysMqpxGGoucGOavhE4P9f+ncgsBRokTQLOBpZExKaI2AwsAeZUsmCpku9mZlZ7yh0WAfxE0nJJC1LbERGxDiA9H57apwBrctu2pLbO2vcjaYGkJklNra2tPSzWx6HMzAqpK/Prvyki1ko6HFgi6cku1i307/foon3/hohFwCKAxsbGHv3V7+ychQqWYGY2eJS1ZxERa9PzBuBHZOcc1qfDS6TnDWn1FmBabvOpwNou2s3MrELKFhaSRksa0zENzAZ+CywGOq5omg/ckaYXAxenq6JmAVvTYap7gNmSxqUT27NTW5/zQSgzs8LKeRjqCOBHys4O1wE3RcTdkh4EbpN0KbAauCCtfxdwDtAM7AQuAYiITZK+ADyY1rsiIjaVo+BOD0P5KJSZDXJlC4uIWAWcVKD9BeCsAu0BfLST1/oW8K2+rtHMzErjX3CXwB0LMxvsHBY5vnTWzKwwh0Wez1mYmRXksCjB2i27ql2CmVlVOSxyOjsI9e1fPlvJMszMao7DokRX/birH5+bmQ1sDouc6GKM8m/+7OkKVmJmVlscFjm+n4WZWWEOCzMzK8phkeNLZM3MCnNY5PgwlJlZYQ6LHGeFmVlhDgszMyvKYZHjw1BmZoU5LHI8kKCZWWEOCzMzK8phkePDUGZmhTkszMysKIdFN/zB5fd0OX6UmdlA5bDIKRYE23e3sX7b7gpVY2ZWOxwWZmZWlMMixweYzMwKc1jk+HSEmVlhDoucUn6U55FpzWwwcljklNKz+PFj68pfiJlZjSl7WEgaKuk3ku5M88dIWiZppaRbJdWn9uFpvjktn557jctS+1OSzi5XrUNK6DZ87j+eYN8+H68ys8GlEj2LvwRW5OavBr4SETOAzcClqf1SYHNEHA98Ja2HpBOBecBrgTnANyQNLUehr508tqT1vvlz34/bzAaXsoaFpKnAucB1aV7AmcDtaZUbgfPT9Nw0T1p+Vlp/LnBLROyOiGeAZuDUctRban/hV0+/UI63NzOrWeXuWVwDfBLYl+YPA7ZERFuabwGmpOkpwBqAtHxrWv/l9gLbvEzSAklNkppaW1t7VGypV0O1+zCUmQ0yZQsLSecBGyJieb65wKpRZFlX27zSELEoIhojonHixIndrjd70dJC4JfuWZjZIFNXxtd+E/BOSecAI4CxZD2NBkl1qfcwFVib1m8BpgEtkuqAQ4FNufYO+W3MzKwCytaziIjLImJqREwnO0F9X0S8F7gfeHdabT5wR5penOZJy++LbLCmxcC8dLXUMcAM4Nflqbkcr2pm1v+Vs2fRmU8Bt0j6W+A3wPWp/Xrgu5KayXoU8wAi4nFJtwFPAG3ARyOivfJlm5kNXhUJi4h4AHggTa+iwNVMEbELuKCT7a8Erixfhel9yv0GZmb9lH/BndOde1VcffeTZazEzKy2OCxyunPO4toH/MM8Mxs8HBY5vguemVlhDoscR4WZWWEOixx3LMzMCnNYmJlZUQ6LHHcszMwKc1jk+AS3mVlhDgszMyvKYZHjjoWZWWEOi5xShyg3MxtsHBY57lmYmRXmsMhxVpiZFeawyHHPwsysMIdFTnfPWSxb5durmtng4LDI6W7P4j2LlpanEDOzGuOwMDOzohwWOVK1KzAzq00Oi5xjJ4yudglmZjXJYWFmZkU5LMzMrCiHhZmZFeWwyHnrCYdXuwQzs5pUUlhIOk7S8DR9hqS/kNRQ3tIqb3jd0G5vs7utvQyVmJnVllJ7Fj8A2iUdD1wPHAPcVLaq+pGv39dc7RLMzMqu1LDYFxFtwJ8A10TEx4FJXW0gaYSkX0t6RNLjkj6f2o+RtEzSSkm3SqpP7cPTfHNaPj33Wpel9qcknd2TD1ou217aW+0SzMzKrtSw2CvpImA+cGdqG1Zkm93AmRFxEjATmCNpFnA18JWImAFsBi5N618KbI6I44GvpPWQdCIwD3gtMAf4hqTuHy8yM7MeKzUsLgFOB66MiGckHQP8W1cbRGZHmh2WHgGcCdye2m8Ezk/Tc9M8aflZkpTab4mI3RHxDNAMnFpi3WX30xUbql2CmVnZlRQWEfFERPxFRNwsaRwwJiKuKradpKGSHgY2AEuAp4Et6ZAWQAswJU1PAdak92sDtgKH5dsLbJN/rwWSmiQ1tba2lvKx+sRzW16q2HuZmVVLqVdDPSBprKTxwCPADZK+XGy7iGiPiJnAVLLewGsKrdbxNp0s66z9wPdaFBGNEdE4ceLEYqWZmVk3lHoY6tCI2Aa8C7ghIt4AvK3UN4mILcADwCygQVJdWjQVWJumW4BpAGn5ocCmfHuBbczMrAJKDYs6SZOAC3nlBHeXJE3s+C2GpJFk4bICuB94d1ptPnBHml6c5knL74uISO3z0tVSxwAzgF+XWHe3XfOemfzh9HHlenkzs36p1LC4ArgHeDoiHpR0LLCyyDaTgPslPQo8CCyJiDuBTwGfkNRMdk7i+rT+9cBhqf0TwEKAiHgcuA14Argb+GhElO2XcOefPIXvf/iN5Xp5M7N+qa74KhAR3we+n5tfBfzPIts8CpxcoH0VBa5miohdwAWdvNaVwJWl1GpmZn2v1BPcUyX9SNIGSesl/UDS1HIX11/4iigzG+hKPQx1A9m5g8lkl63+R2obsP7q7a8qed2tO/0rbjMb2EoNi4kRcUNEtKXHt4EBfX3qyPrSfyTe3Lqj+EpmZv1YqWGxUdL70o/shkp6H/BCOQurNnXjhtx/cfNvyliJmVn1lRoWHyS7bPZ5YB3Zpa2XlKsoMzOrLaUO97E6It4ZERMj4vCIOJ/sB3oDVun9CjOzga83d8r7RJ9VYWZmNa03YTGg//HdjVMWZmYDXm/C4qDB/AYSZ4WZ2Su6/AW3pO0UDgUBI8tSkZmZ1ZwuwyIixlSqkFrTnUtnzcwGut4chhrQnBVmZq9wWJTgxEljq12CmVlVOSw60dGxuLBxKjd96DR+8JGuhy1/aPXm8hdlZlYlDotONIyqB2BKwygaRtXzhqO7viHSu77xy0qUZWZWFSXdz2IwOu/1k2jbt4/zXj+52qWYmVWdw6ITkviTk33LDjMz8GEoMzMrgcOiD+3aW7Zbg5uZVZXDohuGFPntxT/dt7IyhZiZVZjDoht+8vE/7nL5jl1tFarEzKyyHBbdUFeka+EhQsxsoHJYdMP0CaOZ0tD5+InOCjMbqBwW3fThM47rdNne9n0VrMTMrHIcFt30nsZpnS67d8WGClZiZlY5ZQsLSdMk3S9phaTHJf1lah8vaYmklel5XGqXpK9Japb0qKRTcq81P62/UtL8ctVcivq6zneZj0KZ2UBVzp5FG/BXEfEaYBbwUUknAguBeyNiBnBvmgd4BzAjPRYA10IWLsDlwGnAqcDlHQFTa9Zu3VXtEszMyqJsYRER6yLioTS9HVgBTAHmAjem1W4Ezk/Tc4HvRGYp0CBpEnA2sCQiNkXEZmAJMKdcdffW79Zvr3YJZmZ9riLnLCRNB04GlgFHRMQ6yAIFODytNgVYk9usJbV11n7geyyQ1CSpqbW1ta8/QslWv7Czau9tZlYuZQ8LSYcAPwA+FhHbulq1QFt00b5/Q8SiiGiMiMaJEyf2rNgSPXHF2Z0u8+WzZjYQlTUsJA0jC4rvRcQPU/P6dHiJ9NxxCVELkL/UaCqwtov2qhlV3/lgvQ4LMxuIynk1lIDrgRUR8eXcosVAxxVN84E7cu0Xp6uiZgFb02Gqe4DZksalE9uzU1tNkq+JMrMBqJz3s3gT8H7gMUkPp7ZPA1cBt0m6FFgNXJCW3QWcAzQDO4FLACJik6QvAA+m9a6IiE1lrLskh44cxtaX9h7U3rp9dxWqMTMrL0UcdPi/32tsbIympqayvsfSVS8wb9HSgsuevercsr63mVk5SFoeEY2FlvkX3D004ZD6apdgZlYxDoseqh86tNolmJlVjMOih446bFS1SzAzqxiHRRls2O5hP8xsYHFYlMHK9TuqXYKZWZ9yWJTBALzAzMwGOYdFL7xuytiC7f/ru+W9bNfMrNIcFr0wcljhK6Je3NNe4UrMzMrLYdELHtrDzAYLh0VvOCvMbJBwWPSCs8LMBguHhZmZFeWw6AXfu8LMBguHRS90dYL7l80bK1iJmVl5OSx6oauexZ9et6xyhZiZlZnDwszMinJY9EKxcxa79vrHeWY2MDgseuGqd72+y+VPrNtWoUrMzMrLYdEL08aP4p8uOrnaZZiZlZ3DopfOe/0kLmycWnDZnrZ9Fa7GzKw8HBa9JIm5M6cUXDZv0dIKV2NmVh4Oiz4wopPRZ83MBgqHRR845aiGapdgZlZWDos+II/7YWYDnMPCzMyKKltYSPqWpA2SfptrGy9piaSV6Xlcapekr0lqlvSopFNy28xP66+UNL9c9ZqZWefK2bP4NjDngLaFwL0RMQO4N80DvAOYkR4LgGshCxfgcuA04FTg8o6AqTXXvGdmwfb123ZVuBIzs75XtrCIiJ8Dmw5ongvcmKZvBM7PtX8nMkuBBkmTgLOBJRGxKSI2A0s4OIBqwpzXHVmw/W1f/lmFKzEz63uVPmdxRESsA0jPh6f2KcCa3Hotqa2z9oNIWiCpSVJTa2trnxdezIhhQzlmwuiD2rfvamPzi3sqXo+ZWV+qlRPchS4nii7aD26MWBQRjRHROHHixD4trlRzZ04u2H7Hw89VuBIzs75V6bBYnw4vkZ43pPYWYFpuvanA2i7aa9L7Zx1dsL29YLyZmfUflQ6LxUDHFU3zgTty7Renq6JmAVvTYap7gNmSxqUT27NTW00a0snvLZ5u3VHhSszM+lZduV5Y0s3AGcAESS1kVzVdBdwm6VJgNXBBWv0u4BygGdgJXAIQEZskfQF4MK13RUQceNK8ZowZUXh3trtrYWb9nCIG3h+yxsbGaGpqqsp7T1/4nwXbn73q3ApXYmbWPZKWR0RjoWW1coLbzMxqmMOiQlo276x2CWZmPeawqJA3X31/tUswM+sxh0Uf+6MZE6pdgplZn3NY9LHDRtdXuwQzsz7nsOhjXd3bYt++gXflmZkNDg6LPvaRM45j0qEjCi479tN3VbgaM7O+4bDoY686Ygy/uuwsrp9f8FJlM7N+yWFRJme95oiC7e0+FGVm/ZDDooyGFDh98c6v/6LyhZiZ9ZLDoow+9JZjD2p7fO22KlRiZtY7DosyGjtiWLVLMDPrEw6LMjppakPB9j+/6SEG4gCOZjZwOSzK6M0zJvDvH33TQe13PrqOOx9dV4WKzMx6xmFRZjOnFe5dbPJ9uc2sH3FYVMnlix+vdglmZiVzWFTATz/xxwXb12zysOVm1j84LCrg+MMPKdj+R3/nYcvNrH9wWFTILQtmVbsEM7Mec1hUyKxjDyvY3tk9u83MaonDooJWffGcgu1bdvrKKDOrbQ6LChoyRFz5J687qH3mFUvY3dZehYrMzErjsKiwPz31KL75vlMOan/1Z+/2iLRmVrMcFhUmiTmvm8R/ffKtBy077tN3sWuvexhmVnscFlUybfwovvWBg2+QdMJf3822XXurUJGZWef6TVhImiPpKUnNkhZWu56+cOYJR7D0srMOan/9537C9IX/yUt73Msws9qg/jD6qaShwO+AtwMtwIPARRHxRKH1Gxsbo6mpqYIV9k5b+z7+/p6n+Jefr6p2KWbdMrp+KHv3BXva9u3XPnLYUOqGirb24KW97YwYNoSjx4/mkBF1rN60k407dnPCkWOZdOgI1m/bxfNbdzFmRB3TJ4xm7Ihh3P3485z7B5NYuWE7Ew4ZzvLfb+akqQ1s3LGbI8aO4JARdezbF4wdMYyde9uZdOgIDhleR8vmnaxYt53DDqnn0JHDOGLsCIYNFas3vcTR40fxu/XbmdwwkiESJ04ey6YXd9Mwqh6AjTt2M6VhJI+2bKVuiJgybiRDh+jlWw3U1w1hzIg6duxqY1/AnvZ9jBlRx6Yde6ivG8LhY4bz4p42hkg8s/FFpjSMZFfbPsaPqufIQ0ewY3cbo+uHZtsNH8a6rS8B8Py2XYxL62x+cQ9724Nxo4cxur4OgKFDxK697TSMqmd3Wzs7drcxblQ9G3dktW/duZcxI+oYMkSs2/ISJx81jqGF7rxWAknLI6LgPaH7S1icDnwuIs5O85cBRMSXCq3f38Kiw759wb/8fBVX3/1ktUsxs37s2avO7dF2XYVFXa8qqpwpwJrcfAtwWn4FSQuABQBHHXVU5SrrQ0OGiI+ccRwfOeM4ANZtfYnTv3Rflasys/7kk3NeXZbX7S9hUahPtV+XKCIWAYsg61lUoqhym3ToyB7/C8HMrC/1lxPcLcC03PxUYG2VajEzG3T6S1g8CMyQdIykemAesLjKNZmZDRr94jBURLRJ+nPgHmAo8K2I8N2DzMwqpF+EBUBE3AXcVe06zMwGo/5yGMrMzKrIYWFmZkU5LMzMrCiHhZmZFdUvhvvoLkmtwO978RITgI19VM5A4X1SmPfLwbxPCusP++XoiJhYaMGADIvektTU2fgog5X3SWHeLwfzPimsv+8XH4YyM7OiHBZmZlaUw6KwRdUuoAZ5nxTm/XIw75PC+vV+8TkLMzMryj0LMzMrymFhZmZFOSxyJM2R9JSkZkkLq11PX5M0TdL9klZIelzSX6b28ZKWSFqZnseldkn6Wtofj0o6Jfda89P6KyXNz7W/QdJjaZuvSerZzYArTNJQSb+RdGeaP0bSsvT5bk1D4yNpeJpvTsun517jstT+lKSzc+398nslqUHS7ZKeTN+Z0/1dAUkfT////FbSzZJGDIrvS0T4kZ23GQo8DRwL1AOPACdWu64+/oyTgFPS9Bjgd8CJwN8BC1P7QuDqNH0O8GOyOxXOApal9vHAqvQ8Lk2PS8t+DZyetvkx8I5qf+4S980ngJuAO9P8bcC8NP1N4CNp+n8D30zT84Bb0/SJ6TszHDgmfZeG9ufvFXAj8Gdpuh5oGOzfFbJbPD8DjMx9Tz4wGL4v7lm84lSgOSJWRcQe4BZgbpVr6lMRsS4iHkrT24EVZF/+uWR/GEjP56fpucB3IrMUaJA0CTgbWBIRmyJiM7AEmJOWjY2IX0X2f8R3cq9VsyRNBc4FrkvzAs4Ebk+rHLhPOvbV7cBZaf25wC0RsTsingGayb5T/fJ7JWks8BbgeoCI2BMRWxjk35WkDhgpqQ4YBaxjEHxfHBavmAKsyc23pLYBKXWHTwaWAUdExDrIAgU4PK3W2T7pqr2lQHutuwb4JLAvzR8GbImItjSf/xwvf/a0fGtav7v7qtYdC7QCN6TDc9dJGs0g/65ExHPAPwCryUJiK7CcQfB9cVi8otDx0gF5XbGkQ4AfAB+LiG1drVqgLXrQXrMknQdsiIjl+eYCq0aRZQNmnyR1wCnAtRFxMvAi2WGnzgyK/ZLO0cwlO3Q0GRgNvKPAqgPu++KweEULMC03PxVYW6VaykbSMLKg+F5E/DA1r0+HBUjPG1J7Z/ukq/apBdpr2ZuAd0p6lqzLfyZZT6MhHWaA/T/Hy589LT8U2ET391WtawFaImJZmr+dLDwG83cF4G3AMxHRGhF7gR8Cb2QQfF8cFq94EJiRrmqoJzsZtbjKNfWpdKz0emBFRHw5t2gx0HGVynzgjlz7xelKl1nA1nTo4R5gtqRx6V9as4F70rLtkmal97o491o1KSIui4ipETGd7L/5fRHxXuB+4N1ptQP3Sce+endaP1L7vHT1yzHADLITuP3yexURzwNrJL06NZ0FPMEg/q4kq4FZkkalujv2y8D/vlT7DHstPciu6Pgd2dUIn6l2PWX4fG8m69I+CjycHueQHUO9F1iZnsen9QX8c9ofjwGNudf6INlJuWbgklx7I/DbtM3XSaME9IcHcAavXA11LNn/vM3A94HhqX1Emm9Oy4/Nbf+Z9LmfIndlT3/9XgEzgab0ffl3squZBv13Bfg88GSq/btkVzQN+O+Lh/swM7OifBjKzMyKcliYmVlRDgszM9wBfI8AAAHfSURBVCvKYWFmZkU5LMzMrCiHhVkvSPpMGoH0UUkPSzpN0sckjap2bWZ9yZfOmvWQpNOBLwNnRMRuSRPIRgr9JdnvDDZWtUCzPuSehVnPTQI2RsRugBQO7yYbM+h+SfcDSJot6VeSHpL0/TQ2F5KelXS1pF+nx/Gp/YJ0r4RHJP28Oh/NbH/uWZj1UPqj/wuyYap/Snavgp+lcaYaI2Jj6m38kOwXui9K+hTZr3uvSOv9a0RcKeli4MKIOE/SY8CciHhOUkNkQ4ObVZV7FmY9FBE7gDcAC8iG875V0gcOWG0W2Y1u/lvSw2TjBB2dW35z7vn0NP3fwLclfYjsZjhmVVdXfBUz60xEtAMPAA+kHsH8A1YR2c1/LursJQ6cjogPSzqN7IZMD0uaGREv9G3lZt3jnoVZD0l6taQZuaaZwO+B7WS3rQVYCrwpdz5ilKRX5bZ5T+75V2md4yJiWUT8DbCR/YesNqsK9yzMeu4Q4J8kNQBtZCOLLgAuAn4saV1EvDUdmrpZ0vC03WfJRhUFGC5pGdk/3Dp6H3+fQkhkI7s+UpFPY9YFn+A2q5L8ifBq12JWjA9DmZlZUe5ZmJlZUe5ZmJlZUQ4LMzMrymFhZmZFOSzMzKwoh4WZmRX1/wEhtrEiBJq5VQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Autoencoder Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Reduced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'autoencoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-24dff42619ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdataTensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mencoded_data_without_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_act\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mencoded_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_data_without_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m embedded_encoded_data, target = embed_data(\n\u001b[1;32m      5\u001b[0m     \u001b[0mlead_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'autoencoder' is not defined"
     ]
    }
   ],
   "source": [
    "dataTensor = Variable(torch.from_numpy(data[:, 3:].astype(np.float32)))\n",
    "encoded_data_without_date = autoencoder.forward(dataTensor, decode=False, no_act=True).detach().numpy()\n",
    "encoded_data = np.concatenate((data[:, :3], encoded_data_without_date), axis=1)\n",
    "embedded_encoded_data, target = embed_data(\n",
    "    lead_time = 6,\n",
    "    embed_dim = 12,\n",
    "    from_date = date(month=1, year=2006, day=1),\n",
    "    to_date = date(month=3, year=2006, day=31),\n",
    "    data = encoded_data,\n",
    "    t_idx = 2,\n",
    "    dt_idx = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM for forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantileRegression(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size, num_layers, seq_length):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_length = seq_length\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=input_size, \n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, \n",
    "                            batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, 10)\n",
    "        self.fc_out = nn.Linear(10, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_0 = Variable(torch.zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_size))\n",
    "        \n",
    "        c_0 = Variable(torch.zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_size))\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "        # Propagate input through LSTM\n",
    "        ula, (h_out, _) = self.lstm(x, (h_0, c_0))\n",
    "        h_out = h_out.view(batch_size*self.num_layers, self.hidden_size)\n",
    "#         print(ula.size(), h_out.size())\n",
    "#         h_out = h_out.view(-1, self.hidden_size)\n",
    "        \n",
    "#         ula = ula.contiguous().view(x.size(0),-1)\n",
    "        \n",
    "        fc1 = self.fc(h_out)\n",
    "        out = self.fc_out(fc1)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def train_model(self, trainX, trainY, lr, epochs, sample_size):\n",
    "        learning_rate = lr\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        losses = []\n",
    "        num_epochs = epochs\n",
    "        \n",
    "        for epoch in range(num_epochs):  \n",
    "            indices = torch.randperm(trainX.size()[0])\n",
    "            trainX, trainY = trainX[indices, :, :], trainY[indices]\n",
    "            for i in range(0, trainX.size()[0], sample_size):\n",
    "                xx = trainX[i: i + sample_size, :, :]\n",
    "                yy = trainY[i: i + sample_size]\n",
    "                outputs = self.forward(xx)\n",
    "                outputs = outputs.view(self.num_layers, self.output_size, -1)\n",
    "                optimizer.zero_grad()\n",
    "                alpha = 0.1\n",
    "                # obtain the loss function\n",
    "                loss = self.quantile_loss(outputs[-1, :, :], yy, alpha)\n",
    "                losses.append(loss.item())\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "            print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item()))\n",
    "            \n",
    "        return losses\n",
    "    \n",
    "    def train_sgd(self, trainX, trainY, lr, epochs, sample_size):\n",
    "        learning_rate = lr\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        losses = []\n",
    "        num_epochs = epochs\n",
    "        for epoch in range(num_epochs):\n",
    "            for xx, yy in zip(trainX, trainY):\n",
    "                xx = xx.view(1, self.seq_length, self.input_size)\n",
    "                outputs = self.forward(xx)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # obtain the loss function\n",
    "                loss = self.quantile_loss(outputs, yy, alpha=0.1)\n",
    "                losses.append(loss)\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "            print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item()))\n",
    "        return losses\n",
    "    \n",
    "    def quantile_loss(self, output, target, alpha = 0.1):\n",
    "        covered_flag = (output <= target).float()\n",
    "        uncovered_flag = (output > target).float()\n",
    "        return torch.mean((target - output)*(alpha)*covered_flag + (output-target)*(1-alpha)*uncovered_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 0.07024\n",
      "Epoch: 1, loss: 0.08055\n",
      "Epoch: 2, loss: 0.08033\n",
      "Epoch: 3, loss: 0.06548\n",
      "Epoch: 4, loss: 0.05969\n",
      "Epoch: 5, loss: 0.04793\n",
      "Epoch: 6, loss: 0.05669\n",
      "Epoch: 7, loss: 0.04734\n",
      "Epoch: 8, loss: 0.04506\n",
      "Epoch: 9, loss: 0.04397\n",
      "Epoch: 10, loss: 0.05537\n",
      "Epoch: 11, loss: 0.04216\n",
      "Epoch: 12, loss: 0.04154\n",
      "Epoch: 13, loss: 0.05590\n",
      "Epoch: 14, loss: 0.04795\n",
      "Epoch: 15, loss: 0.05201\n",
      "Epoch: 16, loss: 0.04290\n",
      "Epoch: 17, loss: 0.02780\n",
      "Epoch: 18, loss: 0.05297\n",
      "Epoch: 19, loss: 0.04144\n",
      "Epoch: 20, loss: 0.03706\n",
      "Epoch: 21, loss: 0.03277\n",
      "Epoch: 22, loss: 0.04838\n",
      "Epoch: 23, loss: 0.03509\n",
      "Epoch: 24, loss: 0.03992\n",
      "Epoch: 25, loss: 0.03495\n",
      "Epoch: 26, loss: 0.04340\n",
      "Epoch: 27, loss: 0.02282\n",
      "Epoch: 28, loss: 0.04316\n",
      "Epoch: 29, loss: 0.03036\n",
      "Epoch: 30, loss: 0.02681\n",
      "Epoch: 31, loss: 0.03261\n",
      "Epoch: 32, loss: 0.02879\n",
      "Epoch: 33, loss: 0.02570\n",
      "Epoch: 34, loss: 0.03046\n",
      "Epoch: 35, loss: 0.02889\n",
      "Epoch: 36, loss: 0.03090\n",
      "Epoch: 37, loss: 0.03523\n",
      "Epoch: 38, loss: 0.03010\n",
      "Epoch: 39, loss: 0.02723\n"
     ]
    }
   ],
   "source": [
    "regressor_pca = QuantileRegression(\n",
    "    input_size = 10,\n",
    "    hidden_size = 40,\n",
    "    output_size = 1,\n",
    "    num_layers = 1,\n",
    "    seq_length = 12\n",
    ")\n",
    "\n",
    "train_x_pca = embedded_data_pca.astype(np.float32)\n",
    "train_y_pca = target_pca.astype(np.float32)\n",
    "\n",
    "\n",
    "\n",
    "trainX_pca = Variable(torch.from_numpy(train_x_pca))\n",
    "trainY_pca = Variable(torch.from_numpy(train_y_pca))\n",
    "\n",
    "losses_pca = regressor_pca.train_model(\n",
    "    trainX = trainX_pca,\n",
    "    trainY = trainY_pca,\n",
    "    lr = 0.005,\n",
    "    epochs = 40,\n",
    "    sample_size = 50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 0.07941\n",
      "Epoch: 1, loss: 0.07723\n",
      "Epoch: 2, loss: 0.06447\n",
      "Epoch: 3, loss: 0.07276\n",
      "Epoch: 4, loss: 0.06823\n",
      "Epoch: 5, loss: 0.06510\n",
      "Epoch: 6, loss: 0.06061\n",
      "Epoch: 7, loss: 0.05573\n",
      "Epoch: 8, loss: 0.06115\n",
      "Epoch: 9, loss: 0.05808\n",
      "Epoch: 10, loss: 0.06840\n",
      "Epoch: 11, loss: 0.05669\n",
      "Epoch: 12, loss: 0.05826\n",
      "Epoch: 13, loss: 0.05620\n",
      "Epoch: 14, loss: 0.05622\n",
      "Epoch: 15, loss: 0.05999\n",
      "Epoch: 16, loss: 0.05759\n",
      "Epoch: 17, loss: 0.06912\n",
      "Epoch: 18, loss: 0.05695\n",
      "Epoch: 19, loss: 0.05720\n",
      "Epoch: 20, loss: 0.05469\n",
      "Epoch: 21, loss: 0.04773\n",
      "Epoch: 22, loss: 0.04257\n",
      "Epoch: 23, loss: 0.04181\n",
      "Epoch: 24, loss: 0.03924\n",
      "Epoch: 25, loss: 0.05147\n",
      "Epoch: 26, loss: 0.04301\n",
      "Epoch: 27, loss: 0.06202\n",
      "Epoch: 28, loss: 0.04620\n",
      "Epoch: 29, loss: 0.04648\n",
      "Epoch: 30, loss: 0.03612\n",
      "Epoch: 31, loss: 0.04501\n",
      "Epoch: 32, loss: 0.03789\n",
      "Epoch: 33, loss: 0.04524\n",
      "Epoch: 34, loss: 0.03014\n",
      "Epoch: 35, loss: 0.03917\n",
      "Epoch: 36, loss: 0.03988\n",
      "Epoch: 37, loss: 0.03442\n",
      "Epoch: 38, loss: 0.04488\n",
      "Epoch: 39, loss: 0.04769\n"
     ]
    }
   ],
   "source": [
    "regressor = QuantileRegression(\n",
    "    input_size = 25,\n",
    "    hidden_size = 40,\n",
    "    output_size = 1,\n",
    "    num_layers = 1,\n",
    "    seq_length = 12\n",
    ")\n",
    "\n",
    "train_x = embedded_data_original.astype(np.float32)\n",
    "train_y = target_original.astype(np.float32)\n",
    "\n",
    "\n",
    "\n",
    "trainX = Variable(torch.from_numpy(train_x))\n",
    "trainY = Variable(torch.from_numpy(train_y))\n",
    "\n",
    "losses = regressor.train_model(\n",
    "    trainX = trainX,\n",
    "    trainY = trainY,\n",
    "    lr = 0.005,\n",
    "    epochs = 40,\n",
    "    sample_size = 50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_encoded_datax, targetx = embed_data(\n",
    "    lead_time = 6,\n",
    "    embed_dim = 12,\n",
    "    from_date = date(month=5, year=2006, day=1),\n",
    "    to_date = date(month=5, year=2006, day=31),\n",
    "    data = result,\n",
    "    t_idx = 2,\n",
    "    dt_idx = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_encoded_data_originalx, target_originalx = embed_data(\n",
    "    lead_time = 6,\n",
    "    embed_dim = 12,\n",
    "    from_date = date(month=5, year=2006, day=1),\n",
    "    to_date = date(month=5, year=2006, day=31),\n",
    "    data = data,\n",
    "    t_idx = 2,\n",
    "    dt_idx = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_pca = embedded_encoded_datax.astype(np.float32)\n",
    "test_y_pca = targetx.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = embedded_encoded_data_originalx.astype(np.float32)\n",
    "test_y = target_originalx.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_loss_numpy(output, target, alpha):\n",
    "    covered_flag = (output <= target).astype(np.float32)\n",
    "    uncovered_flag = (output > target).astype(np.float32)\n",
    "    return np.mean((target - output)*(alpha)*covered_flag + (output-target)*(1-alpha)*uncovered_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_loss(output, target, alpha = 0.5):\n",
    "        covered_flag = (output <= target).float()\n",
    "        uncovered_flag = (output > target).float()\n",
    "        return torch.mean((target - output)*(alpha)*covered_flag + (output-target)*(1-alpha)*uncovered_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor_pca.eval()\n",
    "testX_pca = Variable(torch.from_numpy(test_x_pca))\n",
    "testY_pca = Variable(torch.from_numpy(test_y_pca))\n",
    "test_predict_pca = regressor_pca(testX_pca)\n",
    "data_predict_pca = test_predict_pca.data.numpy()\n",
    "testY_numpy_pca=testY_pca.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.eval()\n",
    "testX = Variable(torch.from_numpy(test_x))\n",
    "testY = Variable(torch.from_numpy(test_y))\n",
    "test_predict=regressor(testX)\n",
    "data_predict=test_predict.data.numpy()\n",
    "testY_numpy=testY.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15545544"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantile_loss_numpy(data_predict_pca,testY_numpy_pca,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13114303"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantile_loss_numpy(data_predict,testY_numpy,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2628, grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantile_loss(testY,test_predict,0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2706, grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantile_loss(testY_pca,test_predict_pca,0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([744])\n"
     ]
    }
   ],
   "source": [
    "print(testY_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([744])\n"
     ]
    }
   ],
   "source": [
    "print(testY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50376, 28)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50376, 13)\n"
     ]
    }
   ],
   "source": [
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2136, 12, 25) (2136, 12, 10)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape,train_x_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(744, 12, 25) (744, 12, 10)\n"
     ]
    }
   ],
   "source": [
    "print(test_x.shape,test_x_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([744, 1]) torch.Size([744, 1])\n"
     ]
    }
   ],
   "source": [
    "print(test_predict.shape,test_predict_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2136,) (2136,)\n"
     ]
    }
   ],
   "source": [
    "print(train_y.shape,train_y_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
